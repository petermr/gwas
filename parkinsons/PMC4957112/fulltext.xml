<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NPG//DTD XML Article//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName NPG_XML_Article.dtd?><?SourceDTD.Version 2.7.10?><?ConverterInfo.XSLTName nature2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4957112</article-id><article-id pub-id-type="pii">srep30086</article-id><article-id pub-id-type="doi">10.1038/srep30086</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Random Bits Forest: a Strong Classifier/Regressor for Big Data</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yi</given-names></name><xref ref-type="aff" rid="a1">1</xref><xref ref-type="author-notes" rid="n1">*</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yi</given-names></name><xref ref-type="aff" rid="a1">1</xref><xref ref-type="author-notes" rid="n1">*</xref></contrib><contrib contrib-type="author"><name><surname>Pu</surname><given-names>Weilin</given-names></name><xref ref-type="aff" rid="a1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wen</surname><given-names>Kathryn</given-names></name><xref ref-type="aff" rid="a2">2</xref></contrib><contrib contrib-type="author"><name><surname>Shugart</surname><given-names>Yin Yao</given-names></name><xref ref-type="corresp" rid="c1">a</xref><xref ref-type="aff" rid="a2">2</xref></contrib><contrib contrib-type="author"><name><surname>Xiong</surname><given-names>Momiao</given-names></name><xref ref-type="corresp" rid="c2">b</xref><xref ref-type="aff" rid="a3">3</xref></contrib><contrib contrib-type="author"><name><surname>Jin</surname><given-names>Li</given-names></name><xref ref-type="corresp" rid="c3">c</xref><xref ref-type="aff" rid="a1">1</xref></contrib><aff id="a1"><label>1</label><institution>Ministry of Education Key Laboratory of Contemporary Anthropology, Collaborative Innovation Center for Genetics and Development, School of Life Sciences, Fudan University</institution>, Shanghai 200433, <country>China</country></aff><aff id="a2"><label>2</label><institution>Unit on Statistical Genomics, Division of Intramural Division Programs, National Institute of Mental Health, National Institutes of Health</institution>, Bethesda, MD, <country>USA</country></aff><aff id="a3"><label>3</label><institution>Human Genetics Center, School of Public Health, University of Texas Houston Health Sciences Center</institution>, Houston, Texas, <country>USA</country></aff></contrib-group><author-notes><corresp id="c1"><label>a</label><email>yin.yao@nih.gov</email></corresp><corresp id="c2"><label>b</label><email>momiao.xiong@gmail.com</email></corresp><corresp id="c3"><label>c</label><email>lijin@fudan.edu.cn</email></corresp><fn id="n1"><label>*</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>22</day><month>07</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>6</volume><elocation-id>30086</elocation-id><history><date date-type="received"><day>02</day><month>03</month><year>2016</year></date><date date-type="accepted"><day>28</day><month>06</month><year>2016</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2016, Macmillan Publishers Limited</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Macmillan Publishers Limited</copyright-holder><license xmlns:xlink="http://www.w3.org/1999/xlink" license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><!--author-paid--><license-p>This work is licensed under a Creative Commons Attribution 4.0 International License. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in the credit line; if the material is not included under the Creative Commons license, users will need to obtain permission from the license holder to reproduce the material. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions><abstract><p>Efficiency, memory consumption, and robustness are common problems with many popular methods for data analysis. As a solution, we present Random Bits Forest (RBF), a classification and regression algorithm that integrates neural networks (for depth), boosting (for width), and random forests (for prediction accuracy). Through a gradient boosting scheme, it first generates and selects ~10,000 small, 3-layer random neural networks. These networks are then fed into a modified random forest algorithm to obtain predictions. Testing with datasets from the UCI (University of California, Irvine) Machine Learning Repository shows that RBF outperforms other popular methods in both accuracy and robustness, especially with large datasets (N&#x02009;&#x0003e;&#x02009;1000). The algorithm also performed highly in testing with an independent data set, a real psoriasis genome-wide association study (GWAS).</p></abstract></article-meta></front><body><p>The most widely used methods for prediction include linear regressions, logistic regressions, <italic>k</italic>-Nearest Neighbors (<italic>k</italic>-NN)<xref ref-type="bibr" rid="b1">1</xref>, support vector machines (SVM)<xref ref-type="bibr" rid="b2">2</xref>, neural networks (NNs)<xref ref-type="bibr" rid="b3">3</xref>, extreme learning machines (ELM)<xref ref-type="bibr" rid="b4">4</xref>, deep learning (DL)<xref ref-type="bibr" rid="b5">5</xref>, random forests (RF)<xref ref-type="bibr" rid="b6">6</xref><xref ref-type="bibr" rid="b7">7</xref>, and generalized boosted models (GBM)<xref ref-type="bibr" rid="b8">8</xref><xref ref-type="bibr" rid="b9">9</xref>.</p><p>However, each method has its own drawbacks. For instance, linear regression and logistic regression handle linear and log-linear conditions, respectively, but may fail while dealing with nonlinear tasks. <italic>k</italic>-NNs are sensitive to the local structure of the data, with the best choice for <italic>k</italic> dependent on the properties of each datasets<xref ref-type="bibr" rid="b10">10</xref>. SVMs have uncalibrated class membership probabilities, large memory requirements (O(N<sup>2</sup>)), and difficult-to-interpret parameters<xref ref-type="bibr" rid="b2">2</xref><xref ref-type="bibr" rid="b11">11</xref><xref ref-type="bibr" rid="b12">12</xref>. NNs and DL are computationally expensive, with features learnt and tuned iteratively<xref ref-type="bibr" rid="b13">13</xref><xref ref-type="bibr" rid="b14">14</xref>. ELMs do not have sufficient features to handle complex works<xref ref-type="bibr" rid="b15">15</xref>. GBMs have high memory consumption and low evaluation speed<xref ref-type="bibr" rid="b16">16</xref>, as all base-learners must be evaluated in order to obtain predictions for the model. For RFs, decision trees are axis-parallel, which may lead to suboptimal trees; though oblique random forests provide one way to improve the performance of random forests<xref ref-type="bibr" rid="b17">17</xref>, ultimately they may fail on datasets with greater depth<xref ref-type="bibr" rid="b18">18</xref>.</p><p>We created Random Bits Forest (RBF), a classification and regression algorithm that integrates neural networks, boosting, and random forests. We compared the performance of RBF with that of seven other methods, using 28 datasets from the UCI (University of California, Irvine) Machine Learning Repository. We then tested RBF on real psoriasis genome-wide association study (GWAS) data.</p><sec disp-level="1"><title>Methods</title><sec disp-level="2"><title>Summary</title><p>For clarity, features were standardized by subtracting the mean and dividing by standard deviation. The features were then transformed into random features/basis, by gradient boosting of the Random Bits base learner, a 3-layer sparse neural network with random weights, and fed to a random forest classifier/regressor to obtain predictions (<xref ref-type="fig" rid="f1">Fig. 1</xref>).</p></sec><sec disp-level="2"><title>Random Bits</title><p>Our derived feature/basis/base learner is called Random Bits. It is a 3-layer sparse neural network with random weights. Two parameters were used to construct the neural network: <italic>twist1</italic> (the number of features connected to each hidden node) and <italic>twist2</italic> (the number of hidden nodes).</p><p>The features connected with hidden node are randomly assigned and interlayer weights are drawn from a standard normal distribution. The hidden nodes and the top node are the threshold units, with the threshold of each node determined by calculating the linear summation of its input for the <italic>i</italic>th sample z<sub>i</sub> and choosing a random z<sub>i</sub> among the sample as the threshold<xref ref-type="bibr" rid="b15">15</xref>.</p></sec><sec disp-level="2"><title>Boosting Random Bits</title><p>In order to generate many Random Bits, we used a gradient boosting scheme with the following pseudocode:</p><p><bold>For boost</bold>&#x02009;=&#x02009;<bold>1 to B</bold>:</p><p><bold>For step</bold>&#x02009;=&#x02009;<bold>1 to S</bold>:</p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>1</bold>: <bold>residual&#x02009;=&#x02009;Y</bold>; <bold>MaxVar&#x02009;=&#x02009;0</bold>; <bold>BestBit&#x02009;=&#x02009;NULL</bold>;</p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>2</bold>: <bold>For cand&#x02009;=&#x02009;1 to C</bold>:</p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>1</bold>: <bold>Draw a random bit</bold>, <bold>RB</bold></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>2</bold>: <bold>Calculate the residual explained by RB</bold>: <bold>Var</bold></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>3</bold>: <bold>if</bold> (<bold>Var</bold>&#x02009;&#x0003e;&#x02009;<bold>MaxVar</bold>) <bold>{MaxVar&#x02009;=&#x02009;Var</bold>; <bold>BestBit&#x02009;=&#x02009;RB</bold>;}</p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>3</bold>: <bold>Set the random</bold>_<bold>bit</bold>_<bold>pool</bold> [(<bold>boost&#x02009;&#x02212;&#x02009;1</bold>)<bold>&#x02009;</bold>*<bold>&#x02009;S</bold>&#x02009;+&#x02009;<bold>step]&#x02009;=&#x02009;BestBit</bold></p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>4</bold>: <bold>Mean[0]&#x02009;=&#x02009;E</bold>(<bold>residual|BestBit&#x02009;=&#x02009;0</bold>), <bold>Mean[1]&#x02009;=&#x02009;E</bold>(<bold>residual|BestBit&#x02009;=&#x02009;1</bold>)</p><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>5</bold>: <bold>residual&#x02009;=&#x02009;residual&#x02009;&#x02212;&#x02009;Mean[BestBit]</bold>;</p><p>The algorithm launched <italic>B</italic> independent boosting chains, each with <italic>S</italic> steps. Each boosting chain undergoes the standard gradient boosting procedure, starting with a residual of <italic>Y</italic> and updating every step. In each step, <italic>C</italic> Random Bits features (<italic>C</italic>&#x02009;&#x0003e;&#x02009;100) were generated, and the bit with the largest pseudo residual was chosen. The Random Bits from each independent boosting chain were collected to form a large (~10,000) feature pool. The Random Bits were stored in a compressed format requiring 1 bit per Random Bits per sample.</p></sec><sec disp-level="2"><title>Random Bits Forest</title><p>The produced Random Bits are eventually fed to Random Bits Forest. Random Bits Forest is a random forest classifier/regressor, but slightly modified for speed: each tree was grown with a bootstrapped sample and bootstrapped bits, the number of which can be tuned by users. The best bits among all the bootstrapped bits were chosen for each split. By making full use of the binary nature of Random Bits, through special coding and Streaming SIMD Extensions (SSE), acceleration was achieved, such that the modified random forest can afford ~10,000 binary features for large datasets (N&#x02009;=&#x02009;500,000).</p></sec><sec disp-level="2"><title>Benchmarking</title><p>We benchmarked nine methods: linear regression (Linear), logistic regression (LR), <italic>k</italic>-Nearest Neighbors (kNN), neural networks (NN), support vector machines (SVM), extreme learning machines (ELM), random forests (RF), generalized boosted models (GBM), and Random Bits Forest (RBF). We used the RBF software available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://sourceforge.net/projects/random-bits-forest/">http://sourceforge.net/projects/random-bits-forest/</ext-link> and implemented the other eight methods using various R (v3.2.1) packages: stats, RWeka (v0.4-24), nnet (v7.3-8), kernlab (v0.9-19), randomForest (v4.6-10), elmNN (v1.0), and gbm (v2.1). We used ten-fold cross validation (accuracy, sensitivity, specificity and AUC) to evaluate each method&#x02019;s performance. For methods sensitive to parameter selection, we manually tuned the parameters to obtain the best performance. As we chose the best handpicked parameters for each method respectively, the performance of each method based on the best parameters was comparable with each other. The results of tuning the parameters of sensitive methods on the real psoriasis genome-wide association study (GWAS) dataset were provided as <xref ref-type="supplementary-material" rid="S1">Supplemental Materials 1</xref>. Benchmarking was performed on a desktop PC equipped with an AMD FX-8320 CPU and 32GB of memory. SVM, on some large-sample datasets, failed to complete benchmarking within reasonable time (1 week), so those results were left as blank.</p></sec><sec disp-level="2"><title>Benchmarked UCI Datasets Study</title><p>We benchmarked all datasets from the UCI Machine Learning Repository<xref ref-type="bibr" rid="b19">19</xref> that fulfilled the following criteria including: (1) the dataset contains no missing values; (2) the dataset is in dense matrix form; (3) the dataset uses only binary classification; and (4) the dataset had clear instructions and specified the target variable.</p><p>We included 14 regression datasets (<italic><bold>3D Road Network</bold></italic><xref ref-type="bibr" rid="b20">20</xref>, <italic><bold>Bike Sharing</bold></italic><xref ref-type="bibr" rid="b21">21</xref><italic><bold>, Buzz in social media tomhardware</bold></italic>,<italic><bold>Buzz in social media twitter</bold></italic>,<italic><bold>Computer hardware</bold></italic><xref ref-type="bibr" rid="b22">22</xref>, <italic><bold>Concrete compressive strength</bold></italic><xref ref-type="bibr" rid="b23">23</xref>,<italic><bold>Forest fire</bold></italic><xref ref-type="bibr" rid="b24">24</xref>,<italic><bold>Housing</bold></italic><xref ref-type="bibr" rid="b25">25</xref>,<italic><bold>Istanbul stock exchange</bold></italic><xref ref-type="bibr" rid="b26">26</xref>,<italic><bold>Parkinsons telemonitoring</bold></italic><xref ref-type="bibr" rid="b27">27</xref>,<italic><bold>Physicochemical properties of protein tertiary structure, Wine quality</bold></italic><xref ref-type="bibr" rid="b28">28</xref>, <italic><bold>Yacht hydrodynamics</bold></italic><xref ref-type="bibr" rid="b29">29</xref>,<italic><bold>Year prediction MSD</bold></italic>)<xref ref-type="bibr" rid="b30">30</xref> and 14 classification datasets (<italic><bold>Banknote authentication, Blood transfusion service center</bold></italic><xref ref-type="bibr" rid="b31">31</xref>,<italic><bold>Breast cancer wisconsin diagnostic</bold></italic><xref ref-type="bibr" rid="b32">32</xref>,<italic><bold>Climate model simulation crashes</bold></italic><xref ref-type="bibr" rid="b33">33</xref>,<italic><bold>Connectionist bench</bold></italic><xref ref-type="bibr" rid="b34">34</xref>,<italic><bold>EEG eye state, Fertility</bold></italic><xref ref-type="bibr" rid="b35">35</xref>,<italic><bold>Habermans survival</bold></italic><xref ref-type="bibr" rid="b36">36</xref>,<italic><bold>Hill valley with noise</bold></italic><xref ref-type="bibr" rid="b37">37</xref>,<italic><bold>Indian liver patient</bold></italic><xref ref-type="bibr" rid="b38">38</xref>,<italic><bold>Ionosphere</bold></italic><xref ref-type="bibr" rid="b39">39</xref>,<italic><bold>MAGIC gamma telescope</bold></italic><xref ref-type="bibr" rid="b40">40</xref>,<italic><bold>QSAR biodegradation</bold></italic><xref ref-type="bibr" rid="b41">41</xref>,<italic><bold>Skin segmentation</bold></italic>)<xref ref-type="bibr" rid="b42">42</xref>.</p></sec><sec disp-level="2"><title>Applications on GWAS Dataset Study</title><p>We applied each method to a psoriasis genome-wide association (GWAS) genetic dataset<xref ref-type="bibr" rid="b43">43</xref><xref ref-type="bibr" rid="b44">44</xref> to predict disease outcomes. We obtained the dataset, a part of the Collaborative Association Study of Psoriasis (CASP), from the Genetic Association Information Network (GAIN) database, a partnership of the Foundation for the National Institutes of Health. The data were available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://dbgap.ncbi.nlm.nih.gov">http://dbgap.ncbi.nlm.nih.gov</ext-link>. through dbGaP accession number phs000019.v1.p1. All genotypes were filtered by checking for data quality<xref ref-type="bibr" rid="b44">44</xref>. We used 1590 subjects (915 cases, 675 controls) in the general research use (GRU) group and 1133 subjects (431 cases and 702 controls) in the autoimmune disease only (ADO) group. A dermatologist diagnosed all psoriasis cases. Each participant&#x02019;s DNA was genotyped with the Perlegen 500K array. Both cases and controls agreed to sign the consent contract, and controls (&#x02265;18 years old) had no confounding factors relative to a known diagnosis of psoriasis.</p><p>We used both SNP ranking and multiple logistic regression methods, based upon allelic association p-values, for feature selection in training datasets and compared the different methods in both training and testing datasets. First, we trained the model based on the GRU dataset with different numbers of top associated SNPs, and then chose the robust and popular method (LR) to select the best number of SNPs as predictors based on the maximum AUC of the independent ADO (testing) dataset (<xref ref-type="fig" rid="f2">Fig. 2</xref> and <xref ref-type="supplementary-material" rid="S1">Supplemental Materials 2</xref>). We then selected the best number (best number of SNPs&#x02009;=&#x02009;50) of top associated SNPs as input variables and evaluated their performance in both the GRU (training) dataset and independent ADO (testing) dataset for each learning algorithm (except LR). To know more information of these selected 50 top associated SNPs, the Pearson&#x02019;s R squared and Odds Ratio<xref ref-type="bibr" rid="b45">45</xref> were also provided in <xref ref-type="supplementary-material" rid="S1">Supplemental Materials 3</xref>.</p><p>To evaluate a classification method&#x02019;s performance on an imbalanced dataset, we used the area under the receiver operating characteristics (ROC) curve. The area under the curve (AUC) measures the global classification accuracy and is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance<xref ref-type="bibr" rid="b46">46</xref>. We used the AUC as a measure of classifier performance for both GRU (training) and ADO (testing) datasets (<xref ref-type="table" rid="t3">Table 3</xref>, <xref ref-type="fig" rid="f3">Figs 3</xref> and <xref ref-type="fig" rid="f4">4</xref>). The 95% confidence interval (CI) of the AUC<xref ref-type="bibr" rid="b47">47</xref>, sensitivity, specificity and accuracy of all methods were also calculated by choosing the optimal threshold value.</p></sec></sec><sec disp-level="1"><title>Results</title><sec disp-level="2"><title>Results from UCI Datasets Study</title><p><xref ref-type="table" rid="t1">Table 1</xref> shows the regression root-mean-square error (RMSE) of all methods on 14 datasets. RBF was the top performing method in 13 and the second best performing method in 1. In the case (<italic><bold>Housing</bold></italic>) in which RBF was not the best method, the difference between RBF and the top performing method (RF) was within 2%. RF was the second best performing among the regression datasets. RBF&#x02019;s performance exhibited the greatest improvement over that of the other methods with the <italic><bold>3D Road Network</bold></italic> dataset, a shallow task in which the methods predicted the altitude at specific points on a 3D map. However, RBF outperformed RF by allowing non-axis-parallel splitting.</p><p><xref ref-type="table" rid="t2">Table 2</xref> shows the classification error of each method among 14 datasets. RBF was the top performer in 8 datasets, the second best in 5, and the third best for 1. In the cases RBF was not the best method, the difference between RBF and the top performing method was within 2%. SVM was the second best method among classification datasets. RBF&#x02019;s performance exhibited the greatest improvement over that of the other methods with the <italic><bold>Hill valley with noise</bold></italic> dataset, a deep task in which the methods classified the shape (&#x0201c;hill&#x0201d; or &#x0201c;valley&#x0201d;) of a time series with 100 time points. Although all other methods, except neural networks, failed to well perform this task, RBF and its 3-layer random neural network features worked well on this dataset.</p><p>Furthermore, we also observed that the datasets in which RBF performed best were all big datasets (N&#x02009;&#x0003e;&#x02009;1000 with limited features, <xref ref-type="table" rid="t1">Table 1</xref> and <xref ref-type="table" rid="t2">Table 2</xref>). This is due to the nature of trees, which inherently require larger samples than do regressions.</p></sec><sec disp-level="2"><title>Results from GWAS dataset study</title><p><xref ref-type="fig" rid="f2">Figure 2</xref> and <xref ref-type="supplementary-material" rid="S1">Supplemental Materials 2</xref> shows that the ideal number of biomarkers for prediction of psoriasis was 50 in the efficient LR classifier. When the number of biomarkers was less than 20, the AUC of independent ADO (test) dataset was unstable in LR classifier. On the other hand, as the number of biomarkers approached 50, performance improved and stabilized: the best AUC for LR was 0.7063, respectively. Performance did not significantly improve as the number of biomarkers increased over 50.</p><p>As seen in <xref ref-type="table" rid="t3">Table 3</xref>, all benchmarked methods were used to construct effective diagnosis models for psoriasis prediction based on optimal number of SNP subsets. No significant unbalances were found in the training and testing datasets, suggesting the credibility and stability of the prediction models. The average of AUC of 10-fold cross-validation<xref ref-type="bibr" rid="b48">48</xref> in the training dataset and AUC of the independent testing dataset were used to evaluate the performance of all methods. The AUC of each method ranged from 0.6192&#x02212;0.6739 in the training dataset and from 0.6563&#x02212;0.7239 in the testing dataset. We found that RBF, GBM, SVM and RF were the four top performing methods in both the training dataset and the testing dataset. RBF was the top performer in both the training dataset (AUC&#x02009;=&#x02009;0.6739, 95% CI: [0.5254, 0.8275], sensitivity&#x02009;=&#x02009;0.6317, specificity&#x02009;=&#x02009;0.6490, accuracy&#x02009;=&#x02009;0.6390) and the testing dataset (AUC&#x02009;=&#x02009;0.7239, 95% CI: [0.6930, 0.7548], sensitivity&#x02009;=&#x02009;0.6543, specificity&#x02009;=&#x02009;0.7151, accuracy&#x02009;=&#x02009;0.6920). The ROC curves for each method are also shown in <xref ref-type="fig" rid="f3">Fig. 3</xref> and <xref ref-type="fig" rid="f4">Fig. 4</xref> for performance comparison visualization.</p><p>Furthermore, RBF appeared to be robust in sensitivity and specificity in both the training and testing datasets. Although the sensitivity and specificity of RBF were not the best for all datasets, its AUC still was the top performer in both GRU (training) and ADO (testing) datasets. This characteristic of RBF is also applicable in the unbalanced dataset, whose prediction performance may be easily influenced by the disease population ratio. In <xref ref-type="table" rid="t3">Table 3</xref>, we see that although KNN has the second accuracy (accuracy&#x02009;=&#x02009;0.6884) in the testing dataset, its AUC performance (AUC&#x02009;=&#x02009;0.7021) is poor because it pays more attention to specificity (specificity&#x02009;=&#x02009;0.7279) than sensitivity (sensitivity = 0.6241).</p></sec></sec><sec disp-level="1"><title>Discussion</title><p>Random forests are among the top performing algorithms for machine learning, as they are accurate, fast, flexible, and mature. Random forest<xref ref-type="bibr" rid="b6">6</xref> is a substantial modification of bagging which builds a large number of de-correlated trees and then averages the trees. The main idea of random forests is to improve the variance reduction of bagging by reducing the correlation between trees without increasing the variance heavily<xref ref-type="bibr" rid="b49">49</xref>. And the target is achieved in the tree-growing process by randomly selecting the input variables. Thus, Random Bits Forest mainly focuses on the automated feature engineering of random forests. We also obtain good results if we feed random bits to a regularized linear regression, though, in big data cases, no better than we get from random forests. And the statistical inference<xref ref-type="bibr" rid="b50">50</xref> of random forests equally applies to RBF.</p><p>RBF outperforms the random forest algorithm by breaking its two limitations: the limitation to axis-parallel splitting that may lead to suboptimal trees<xref ref-type="bibr" rid="b17">17</xref>, and the decision tree depth of two that could fail on dataset with greater depth<xref ref-type="bibr" rid="b18">18</xref>. To overcome the first limitation, we used random projections. Because of pre-generation of many (~10,000) random projections, the tree is allowed to grow with more freedom. To overcome the second limitation, we improved na&#x000ef;ve random projections with a 3-layer random neural network. We then defined a random neural network based on the original features and took its output as a derived feature/basis. Such additional depth may be crucial for specific datasets (UCI dataset: <italic><bold>Hill valley with noise</bold></italic>, shown in <xref ref-type="table" rid="t2">Table 2</xref>).</p><p>Compared to oblique random forests, RBF generated non-axis parallel features before random forest while oblique random forests generates oblique splits within the tree-growing process. One crucial improvement to our random projections was to use 3-layer random neural networks as random projection/basis, giving the random forest more depth. Additional layers did not improve accuracy on the benchmarked datasets, potentially because 3-layer neural networks are already universal approximations.</p><p>In order to make full use of our ~10,000 bits budget, we need a feature selection procedure rather than na&#x000ef;ve random projections. Feature selection was achieved by employing the gradient boosting framework. Instead of directly using the boosting predictions, we collected the boosted basis and fed them into the random forest. First, we found the random bit that best explained the residual and subtracted its effect from the residual to avoid highly correlated random bits. For the <italic><bold>Hill valley with noise</bold></italic> dataset, this method for feature selection reduced error from 11% to 2.5%, compared with na&#x000ef;ve random projections.</p><p>In the boosting procedure, we used multiple independent boost chains, originally just for ease of parallel computing. However, multiple chains also reduced the local optimum problem and led to better prediction. For small datasets, 256 boost chains were used.</p><p>Large sample (N&#x02009;&#x0003e;&#x02009;1000) are important for the success of RBF since trees are more flexible models than are linear models and as a result require a larger sample size. For smaller samples, regularization is useful, which was achieved by limiting the bootstrapped sample size. The consequence is that each tree was suboptimal and biased, but the trees are further decorrelated, thus reducing variance. Reducing feature bootstrap also helped to regularize the problem.</p><p>In summary, we firstly present Random Bits Forest (RBF), an original classification and regression algorithm that integrates the advantages of neural networks (for learning depth), boosting (for learning width), and random forests (for prediction accuracy). That is the reason why Random Bits Forest will perform better than other methods.</p><p>In conclusion, RBF is a novel robust method for machine learning, which is especially effective in datasets with large sample sizes (N&#x02009;&#x0003e;&#x02009;1000). Our work indicates that RBF performs better if fed with extracted/selected features by using appropriate feature selection methods.</p></sec><sec disp-level="1"><title>Additional Information</title><p><bold>How to cite this article</bold>: Wang, Y. <italic>et al.</italic> Random Bits Forest: a Strong Classifier/Regressor for Big Data. <italic>Sci. Rep.</italic>
<bold>6</bold>, 30086; doi: 10.1038/srep30086 (2016).</p></sec><sec sec-type="supplementary-material" id="S1"><title>Supplementary Material</title><supplementary-material id="d33e23" content-type="local-data"><caption><title>Supplemental Materials 1</title></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep30086-s1.xls"/></supplementary-material><supplementary-material id="d33e26" content-type="local-data"><caption><title>Supplemental Materials 2</title></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep30086-s2.xls"/></supplementary-material><supplementary-material id="d33e29" content-type="local-data"><caption><title>Supplemental Materials 3</title></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep30086-s3.xls"/></supplementary-material></sec></body><back><ack><p>The computations involved in this study were supported by the Fudan University High-End Computing Center. The views expressed in this presentation do not necessarily represent the views of the NIMH, NIH, HHS or the United States Government.</p></ack><ref-list><ref id="b1"><mixed-citation publication-type="journal"><name><surname>Altman</surname><given-names>N. S.</given-names></name>
<article-title>An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression</article-title>. <source>The American Statistician</source>
<volume>46</volume>, <fpage>175</fpage>, doi: <pub-id pub-id-type="doi">10.2307/2685209</pub-id> (<year>1992</year>).</mixed-citation></ref><ref id="b2"><mixed-citation publication-type="journal"><name><surname>Cortes</surname><given-names>C.</given-names></name> &#x00026; <name><surname>Vapnik</surname><given-names>V.</given-names></name>
<article-title>Support-vector networks</article-title>. <source>Machine learning</source>
<volume>20</volume>, <fpage>273</fpage>&#x02013;<lpage>297</lpage> (<year>1995</year>).</mixed-citation></ref><ref id="b3"><mixed-citation publication-type="journal"><name><surname>Ripley</surname><given-names>B. D.</given-names></name>
<source>Pattern recognition and neural networks</source>. (Cambridge university press, <year>1996</year>).</mixed-citation></ref><ref id="b4"><mixed-citation publication-type="other"><name><surname>Huang</surname><given-names>G.-B.</given-names></name>, <name><surname>Zhu</surname><given-names>Q.-Y.</given-names></name> &#x00026; <name><surname>Siew</surname><given-names>C.-K.</given-names></name> In <italic>Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on.</italic> 985&#x02013;990 (IEEE).</mixed-citation></ref><ref id="b5"><mixed-citation publication-type="other"><name><surname>Bengio</surname><given-names>Y.</given-names></name>
<article-title>Learning deep architectures for AI</article-title>. Foundations and trends<sup>&#x000ae;</sup> in Machine Learning <volume>2</volume>, <fpage>1</fpage>&#x02013;<lpage>127</lpage> (<year>2009</year>).</mixed-citation></ref><ref id="b6"><mixed-citation publication-type="journal"><name><surname>Breiman</surname><given-names>L.</given-names></name>
<article-title>Random forests</article-title>. <source>Machine learning</source>
<volume>45</volume>, <fpage>5</fpage>&#x02013;<lpage>32</lpage> (<year>2001</year>).</mixed-citation></ref><ref id="b7"><mixed-citation publication-type="journal"><name><surname>Liaw</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Wiener</surname><given-names>M.</given-names></name>
<article-title>Classification and regression by randomForest</article-title>. <source>R news</source>
<volume>2</volume>, <fpage>18</fpage>&#x02013;<lpage>22</lpage> (<year>2002</year>).</mixed-citation></ref><ref id="b8"><mixed-citation publication-type="journal"><name><surname>Freund</surname><given-names>Y.</given-names></name> &#x00026; <name><surname>Schapire</surname><given-names>R. E.</given-names></name>
<article-title>A decision-theoretic generalization of on-line learning and an application to boosting</article-title>. <source>Journal of computer and system sciences</source>
<volume>55</volume>, <fpage>119</fpage>&#x02013;<lpage>139</lpage> (<year>1997</year>).</mixed-citation></ref><ref id="b9"><mixed-citation publication-type="journal"><name><surname>Friedman</surname><given-names>J. H.</given-names></name>
<article-title>Greedy function approximation: a gradient boosting machine</article-title>. <source>Annals of statistics</source>, <fpage>1189</fpage>&#x02013;<lpage>1232</lpage> (<year>2001</year>).</mixed-citation></ref><ref id="b10"><mixed-citation publication-type="other"><name><surname>Phyu</surname><given-names>T. N.</given-names></name> In <italic>Proceedings of the International MultiConference of Engineers and Computer Scientists</italic>. 18&#x02013;20.</mixed-citation></ref><ref id="b11"><mixed-citation publication-type="journal"><name><surname>Burges</surname><given-names>C. J.</given-names></name>
<article-title>A tutorial on support vector machines for pattern recognition</article-title>. <source>Data mining and knowledge discovery</source>
<volume>2</volume>, <fpage>121</fpage>&#x02013;<lpage>167</lpage> (<year>1998</year>).</mixed-citation></ref><ref id="b12"><mixed-citation publication-type="journal"><name><surname>Platt</surname><given-names>J.</given-names></name>
<article-title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</article-title>. <source>Advances in large margin classifiers</source>
<volume>10</volume>, <fpage>61</fpage>&#x02013;<lpage>74</lpage> (<year>1999</year>).</mixed-citation></ref><ref id="b13"><mixed-citation publication-type="journal"><name><surname>Bengio</surname><given-names>Y.</given-names></name>, <name><surname>Courville</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Vincent</surname><given-names>P.</given-names></name>
<article-title>Representation learning: A review and new perspectives</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>
<volume>35</volume>, <fpage>1798</fpage>&#x02013;<lpage>1828</lpage> (<year>2013</year>).</mixed-citation></ref><ref id="b14"><mixed-citation publication-type="journal"><name><surname>Tu</surname><given-names>J. V.</given-names></name>
<article-title>Advantages and disadvantages of using artificial neural networks versus logistic regression for predicting medical outcomes</article-title>. <source>Journal of clinical epidemiology</source>
<volume>49</volume>, <fpage>1225</fpage>&#x02013;<lpage>1231</lpage> (<year>1996</year>).<pub-id pub-id-type="pmid">8892489</pub-id></mixed-citation></ref><ref id="b15"><mixed-citation publication-type="other"><name><surname>Wang</surname><given-names>Y.</given-names></name>, <name><surname>Li</surname><given-names>Y.</given-names></name>, <name><surname>Xiong</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Jin</surname><given-names>L.</given-names></name> Random Bits Regression: a Strong General Predictor for Big Data. <italic>arXiv preprint arXiv:1501.02990</italic> (2015).</mixed-citation></ref><ref id="b16"><mixed-citation publication-type="journal"><name><surname>Natekin</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Knoll</surname><given-names>A.</given-names></name>
<article-title>Gradient boosting machines, a tutorial</article-title>. <source>Front Neurorobot</source>
<volume>7</volume>, <fpage>21</fpage>, doi: <pub-id pub-id-type="doi">10.3389/fnbot.2013.00021</pub-id> (<year>2013</year>).<pub-id pub-id-type="pmid">24409142</pub-id></mixed-citation></ref><ref id="b17"><mixed-citation publication-type="journal"><name><surname>Menze</surname><given-names>B. H.</given-names></name>, <name><surname>Kelm</surname><given-names>B. M.</given-names></name>, <name><surname>Splitthoff</surname><given-names>D. N.</given-names></name>, <name><surname>Koethe</surname><given-names>U.</given-names></name> &#x00026; <name><surname>Hamprecht</surname><given-names>F. A.</given-names></name> In <source>Machine Learning and Knowledge Discovery in Databases</source>
<fpage>453</fpage>&#x02013;<lpage>469</lpage> (Springer, <year>2011</year>).</mixed-citation></ref><ref id="b18"><mixed-citation publication-type="journal"><name><surname>Bengio</surname><given-names>Y.</given-names></name>, <name><surname>Delalleau</surname><given-names>O.</given-names></name> &#x00026; <name><surname>Simard</surname><given-names>C.</given-names></name>
<article-title>Decision trees do not generalize to new variations</article-title>. <source>Computational Intelligence</source>
<volume>26</volume>, <fpage>449</fpage>&#x02013;<lpage>467</lpage> (<year>2010</year>).</mixed-citation></ref><ref id="b19"><mixed-citation publication-type="other">Bache, K. &#x00026; Lichman, M. UCI machine learning repository (<year>2013</year>).</mixed-citation></ref><ref id="b20"><mixed-citation publication-type="other"><name><surname>Kaul</surname><given-names>M.</given-names></name>, <name><surname>Yang</surname><given-names>B.</given-names></name> &#x00026; <name><surname>Jensen</surname><given-names>C. S.</given-names></name> In <italic>Mobile Data Management (MDM), 2013 IEEE 14th International Conference on</italic>. 137&#x02013;146 (IEEE).</mixed-citation></ref><ref id="b21"><mixed-citation publication-type="journal"><name><surname>Fanaee-T</surname><given-names>H.</given-names></name> &#x00026; <name><surname>Gama</surname><given-names>J.</given-names></name>
<article-title>Event labeling combining ensemble detectors and background knowledge</article-title>. <source>Progress in Artificial Intelligence</source>, <fpage>1</fpage>&#x02013;<lpage>15</lpage>, doi: <pub-id pub-id-type="doi">10.1007/s13748-013-0040-3</pub-id> (<year>2013</year>).</mixed-citation></ref><ref id="b22"><mixed-citation publication-type="journal"><name><surname>Kibler</surname><given-names>D.</given-names></name>, <name><surname>Aha</surname><given-names>D. W.</given-names></name> &#x00026; <name><surname>Albert</surname><given-names>M. K.</given-names></name>
<article-title>Instance&#x02010;based prediction of real&#x02010;valued attributes</article-title>. <source>Computational Intelligence</source>
<volume>5</volume>, <fpage>51</fpage>&#x02013;<lpage>57</lpage> (<year>1989</year>).</mixed-citation></ref><ref id="b23"><mixed-citation publication-type="journal"><name><surname>Yeh</surname><given-names>I.-C.</given-names></name>
<article-title>Modeling of strength of high-performance concrete using artificial neural networks</article-title>. <source>Cement and Concrete research</source>
<volume>28</volume>, <fpage>1797</fpage>&#x02013;<lpage>1808</lpage> (<year>1998</year>).</mixed-citation></ref><ref id="b24"><mixed-citation publication-type="journal"><name><surname>Cortez</surname><given-names>P.</given-names></name> &#x00026; <name><surname>Morais</surname><given-names>A.</given-names></name> In <source>Proc. EPIA</source> (eds <name><surname>Neves</surname><given-names>J.</given-names></name>, <name><surname>Santos</surname><given-names>M. F.</given-names></name> &#x00026; <name><surname>Machado</surname><given-names>J.</given-names></name> ), <fpage>512</fpage>&#x02013;<lpage>523</lpage> (<year>2007</year>).</mixed-citation></ref><ref id="b25"><mixed-citation publication-type="other"><name><surname>Belsley</surname><given-names>David A.</given-names></name>, <name><surname>Roy</surname><given-names>E. K.</given-names></name> &#x00026; <name><surname>Welsch</surname><given-names>E.</given-names></name> Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. (<year>2005</year>).</mixed-citation></ref><ref id="b26"><mixed-citation publication-type="journal"><name><surname>Akbilgic</surname><given-names>O.</given-names></name>, <name><surname>Bozdogan</surname><given-names>H.</given-names></name> &#x00026; <name><surname>Balaban</surname><given-names>M. E.</given-names></name>
<article-title>A novel Hybrid RBF Neural Networks model as a forecaster</article-title>. <source>Statistics and Computing</source>
<volume>24</volume>, <fpage>365</fpage>&#x02013;<lpage>375</lpage>, doi: <pub-id pub-id-type="doi">10.1007/s11222-013-9375-7</pub-id> (<year>2013</year>).</mixed-citation></ref><ref id="b27"><mixed-citation publication-type="journal"><name><surname>Tsanas</surname><given-names>A.</given-names></name>, <name><surname>Little</surname><given-names>M. A.</given-names></name>, <name><surname>McSharry</surname><given-names>P. E.</given-names></name> &#x00026; <name><surname>Ramig</surname><given-names>L. O.</given-names></name>
<article-title>Accurate telemonitoring of Parkinson&#x02019;s disease progression by noninvasive speech tests</article-title>. <source>IEEE transactions on bio-medical engineering</source>
<volume>57</volume>, <fpage>884</fpage>&#x02013;<lpage>893</lpage>, doi: <pub-id pub-id-type="doi">10.1109/TBME.2009.2036000</pub-id> (<year>2010</year>).<pub-id pub-id-type="pmid">19932995</pub-id></mixed-citation></ref><ref id="b28"><mixed-citation publication-type="journal"><name><surname>Cortez</surname><given-names>P.</given-names></name>, <name><surname>Cerdeira</surname><given-names>A.</given-names></name>, <name><surname>Almeida</surname><given-names>F.</given-names></name>, <name><surname>Matos</surname><given-names>T.</given-names></name> &#x00026; <name><surname>Reis</surname><given-names>J.</given-names></name>
<article-title>Modeling wine preferences by data mining from physicochemical properties</article-title>. <source>Decision Support Systems</source>
<volume>47</volume>, <fpage>547</fpage>&#x02013;<lpage>553</lpage> (<year>2009</year>).</mixed-citation></ref><ref id="b29"><mixed-citation publication-type="journal"><name><surname>Gerritsma</surname><given-names>J.</given-names></name>, <name><surname>Onnink</surname><given-names>R.</given-names></name> &#x00026; <name><surname>Versluis</surname><given-names>A.</given-names></name>
<source>Geometry, resistance and stability of the delft systematic yacht hull series</source>. (Delft University of Technology, <year>1981</year>).</mixed-citation></ref><ref id="b30"><mixed-citation publication-type="other"><name><surname>Bertin-Mahieux</surname><given-names>T.</given-names></name>, <name><surname>Ellis</surname><given-names>D. P.</given-names></name>, <name><surname>Whitman</surname><given-names>B.</given-names></name> &#x00026; <name><surname>Lamere</surname><given-names>P.</given-names></name> In ISMIR 2011: Proceedings of the 12th International Society for Music Information Retrieval Conference, October 24&#x02013;28, Miami, Florida. 591&#x02013;596 (University of Miami) (<year>2011</year>).</mixed-citation></ref><ref id="b31"><mixed-citation publication-type="journal"><name><surname>Yeh</surname><given-names>I. C.</given-names></name>, <name><surname>Yang</surname><given-names>K.-J.</given-names></name> &#x00026; <name><surname>Ting</surname><given-names>T.-M.</given-names></name>
<article-title>Knowledge discovery on RFM model using Bernoulli sequence</article-title>. <source>Expert Systems with Applications</source>
<volume>36</volume>, <fpage>5866</fpage>&#x02013;<lpage>5871</lpage>, doi: <pub-id pub-id-type="doi">10.1016/j.eswa.2008.07.018</pub-id> (<year>2009</year>).</mixed-citation></ref><ref id="b32"><mixed-citation publication-type="other"><name><surname>Street</surname><given-names>W. N.</given-names></name>, <name><surname>Wolberg</surname><given-names>W. H.</given-names></name> &#x00026; <name><surname>Mangasaria</surname><given-names>O. L.</given-names></name> In <italic>International Symposium on Electronic Imaging: Science and Technology</italic>. 861&#x02013;870.</mixed-citation></ref><ref id="b33"><mixed-citation publication-type="journal"><name><surname>Lucas</surname><given-names>D.</given-names></name>
<italic>et al.</italic>
<article-title>Failure analysis of parameter-induced simulation crashes in climate models</article-title>. <source>Geoscientific Model Development</source>
<volume>6</volume>, <fpage>1157</fpage>&#x02013;<lpage>1171</lpage> (<year>2013</year>).</mixed-citation></ref><ref id="b34"><mixed-citation publication-type="journal"><name><surname>Gorman</surname><given-names>R. P.</given-names></name> &#x00026; <name><surname>Sejnowski</surname><given-names>T. J.</given-names></name>
<article-title>Analysis of hidden units in a layered network trained to classify sonar targets</article-title>. <source>Neural networks</source>
<volume>1</volume>, <fpage>75</fpage>&#x02013;<lpage>89</lpage> (<year>1988</year>).</mixed-citation></ref><ref id="b35"><mixed-citation publication-type="journal"><name><surname>Gil</surname><given-names>D.</given-names></name>, <name><surname>Girela</surname><given-names>J. L.</given-names></name>, <name><surname>De Juan</surname><given-names>J.</given-names></name>, <name><surname>Gomez-Torres</surname><given-names>M. J.</given-names></name> &#x00026; <name><surname>Johnsson</surname><given-names>M.</given-names></name>
<article-title>Predicting seminal quality with artificial intelligence methods</article-title>. <source>Expert Systems with Applications</source>
<volume>39</volume>, <fpage>12564</fpage>&#x02013;<lpage>12573</lpage> (<year>2012</year>).</mixed-citation></ref><ref id="b36"><mixed-citation publication-type="other"><name><surname>Haberman</surname><given-names>S. J.</given-names></name> In <italic>Proceedings of the 9th International Biometrics Conference</italic>. 104&#x02013;122.</mixed-citation></ref><ref id="b37"><mixed-citation publication-type="journal"><name><surname>Hall</surname><given-names>M.</given-names></name>
<italic>et al.</italic>
<article-title>The WEKA data mining software: an update</article-title>. <source>ACM SIGKDD explorations newsletter</source>
<volume>11</volume>, <fpage>10</fpage>&#x02013;<lpage>18</lpage> (<year>2009</year>).</mixed-citation></ref><ref id="b38"><mixed-citation publication-type="journal"><name><surname>Ramana</surname><given-names>B. V.</given-names></name>, <name><surname>Babu</surname><given-names>M. S. P.</given-names></name> &#x00026; <name><surname>Venkateswarlu</surname><given-names>N.</given-names></name>
<article-title>A critical comparative study of liver patients from usa and india: An exploratory analysis</article-title>. <source>International Journal of Computer Science Issues</source>
<volume>9</volume>, <fpage>506</fpage>&#x02013;<lpage>516</lpage> (<year>2012</year>).</mixed-citation></ref><ref id="b39"><mixed-citation publication-type="journal"><name><surname>Sigillito</surname><given-names>V. G.</given-names></name>, <name><surname>Wing</surname><given-names>S. P.</given-names></name>, <name><surname>Hutton</surname><given-names>L. V.</given-names></name> &#x00026; <name><surname>Baker</surname><given-names>K. B.</given-names></name><article-title> Classification of radar returns from the ionosphere using neural networks</article-title>. <source>Johns Hopkins APL Technical Digest</source>
<volume>10</volume>, <fpage>262</fpage>&#x02013;<lpage>266</lpage> (<year>1989</year>).</mixed-citation></ref><ref id="b40"><mixed-citation publication-type="journal"><name><surname>Bock</surname><given-names>R.</given-names></name>
<italic>et al.</italic>
<article-title>Methods for multidimensional event classification: a case study using images from a Cherenkov gamma-ray telescope</article-title>. <source>Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</source>
<volume>516</volume>, <fpage>511</fpage>&#x02013;<lpage>528</lpage> (<year>2004</year>).</mixed-citation></ref><ref id="b41"><mixed-citation publication-type="journal"><name><surname>Mansouri</surname><given-names>K.</given-names></name>, <name><surname>Ringsted</surname><given-names>T.</given-names></name>, <name><surname>Ballabio</surname><given-names>D.</given-names></name>, <name><surname>Todeschini</surname><given-names>R.</given-names></name> &#x00026; <name><surname>Consonni</surname><given-names>V.</given-names></name>
<article-title>Quantitative structure&#x02013;activity relationship models for ready biodegradability of chemicals</article-title>. <source>Journal of chemical information and modeling</source>
<volume>53</volume>, <fpage>867</fpage>&#x02013;<lpage>878</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23469921</pub-id></mixed-citation></ref><ref id="b42"><mixed-citation publication-type="journal"><name><surname>Mattern</surname><given-names>W. D.</given-names></name>, <name><surname>Sommers</surname><given-names>S. C.</given-names></name> &#x00026; <name><surname>Kassirer</surname><given-names>J. P.</given-names></name>
<article-title>Oliguric acute renal failure in malignant hypertension</article-title>. <source>Am J Med.</source>
<volume>52</volume>, <fpage>187</fpage>&#x02013;<lpage>197</lpage> (<year>1972</year>).<pub-id pub-id-type="pmid">5058504</pub-id></mixed-citation></ref><ref id="b43"><mixed-citation publication-type="journal"><name><surname>Fang</surname><given-names>S.</given-names></name>, <name><surname>Fang</surname><given-names>X.</given-names></name> &#x00026; <name><surname>Xiong</surname><given-names>M.</given-names></name>
<article-title>Psoriasis prediction from genome-wide SNP profiles</article-title>. <source>BMC Dermatol</source>
<volume>11</volume>, <fpage>1</fpage>, doi: <pub-id pub-id-type="doi">10.1186/1471-5945-11-1</pub-id> (<year>2011</year>).<pub-id pub-id-type="pmid">21214922</pub-id></mixed-citation></ref><ref id="b44"><mixed-citation publication-type="journal"><name><surname>Nair</surname><given-names>R. P.</given-names></name>
<italic>et al.</italic>
<article-title>Sequence and haplotype analysis supports HLA-C as the psoriasis susceptibility 1 gene</article-title>. <source>The American Journal of Human Genetics</source>
<volume>78</volume>, <fpage>827</fpage>&#x02013;<lpage>851</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">16642438</pub-id></mixed-citation></ref><ref id="b45"><mixed-citation publication-type="journal"><name><surname>Clarke</surname><given-names>G. M.</given-names></name>
<italic>et al.</italic>
<article-title>Basic statistical analysis in genetic case-control studies</article-title>. <source>Nat Protoc</source>
<volume>6</volume>, <fpage>121</fpage>&#x02013;<lpage>133</lpage>, doi: <pub-id pub-id-type="doi">10.1038/nprot.2010.182</pub-id> (<year>2011</year>).<pub-id pub-id-type="pmid">21293453</pub-id></mixed-citation></ref><ref id="b46"><mixed-citation publication-type="journal"><name><surname>Fawcett</surname><given-names>T.</given-names></name>
<article-title>An introduction to ROC analysis</article-title>. <source>Pattern recognition letters</source>
<volume>27</volume>, <fpage>861</fpage>&#x02013;<lpage>874</lpage> (<year>2006</year>).</mixed-citation></ref><ref id="b47"><mixed-citation publication-type="journal"><name><surname>DeLong</surname><given-names>E. R.</given-names></name>, <name><surname>DeLong</surname><given-names>D. M.</given-names></name> &#x00026; <name><surname>Clarke-Pearson</surname><given-names>D. L.</given-names></name>
<article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title>. <source>Biometrics</source>
<volume>44</volume>, <fpage>837</fpage>&#x02013;<lpage>845</lpage> (<year>1988</year>).<pub-id pub-id-type="pmid">3203132</pub-id></mixed-citation></ref><ref id="b48"><mixed-citation publication-type="other"><name><surname>Kohavi</surname><given-names>R.</given-names></name> In <italic>Ijcai.</italic> 1137&#x02013;1145.</mixed-citation></ref><ref id="b49"><mixed-citation publication-type="other"><name><surname>Trevor Hastie</surname><given-names>R. T.</given-names></name> Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second Edition edn. (2009).</mixed-citation></ref><ref id="b50"><mixed-citation publication-type="journal"><name><surname>Boulesteix</surname><given-names>A. L.</given-names></name>, <name><surname>Janitza</surname><given-names>S.</given-names></name>, <name><surname>Kruppa</surname><given-names>J.</given-names></name> &#x00026; <name><surname>K&#x000f6;nig</surname><given-names>I. R.</given-names></name>
<article-title>Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics</article-title>. <source>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</source>
<volume>2</volume>, <fpage>493</fpage>&#x02013;<lpage>507</lpage> (<year>2012</year>).</mixed-citation></ref></ref-list><fn-group><fn><p><bold>Author Contributions</bold> Y.W., Y.L. and L.J. conceived the idea, proposed the RBF method, and contributed to writing of the paper. Y.W., Y.L., Y.Y.S. and L.J. contributed the theoretical analysis. Y.W. also contributed to the development of RBF software using C++. Y.L. helped maintain RBF software and used R to generate tables and figures for all simulated and real datasets. W.P. and Y.L. used the R package &#x02018;ggplot2&#x02019; to plot figures. MMX helped support the psoriasis GWAS dataset and revise the paper. Y.Y.S. and K.W. contributed to scientific discussion and manuscript writing. L.J. contributed to final revision of the paper.</p></fn></fn-group></back><floats-group><fig id="f1"><label>Figure 1</label><caption><title>The summarized process.</title><p>A 3-layer sparse neural network with random weights. <italic><bold>Z</bold></italic> represents threshold functions.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep30086-f1"/></fig><fig id="f2"><label>Figure 2</label><caption><title>Maximum AUC of the independent ADO testing dataset with different numbers of markers.</title></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep30086-f2"/></fig><fig id="f3"><label>Figure 3</label><caption><title>The ROC curve of six best benchmarked methods on the Psoriasis GWAS dataset of independent ADO group using selected best number of SNPs.</title></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep30086-f3"/></fig><fig id="f4"><label>Figure 4</label><caption><title>The average of ten-fold&#x02019;s cross-validation ROC curve of six best benchmarked methods on the Psoriasis GWAS dataset of GRU group using selected best number of SNPs.</title></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep30086-f4"/></fig><table-wrap position="float" id="t1"><label>Table 1</label><caption><title>Regression RMSE of all methods on 14 datasets.</title></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/></colgroup><thead valign="bottom"><tr><th align="left" valign="top" charoff="50">Regression RMSE</th><th align="center" valign="top" charoff="50">Sample</th><th align="center" valign="top" charoff="50">Feature</th><th align="center" valign="top" charoff="50">Linear</th><th align="center" valign="top" charoff="50">KNN</th><th align="center" valign="top" charoff="50">NN</th><th align="center" valign="top" charoff="50">ELM</th><th align="center" valign="top" charoff="50">SVM</th><th align="center" valign="top" charoff="50">GBM</th><th align="center" valign="top" charoff="50">RF</th><th align="center" valign="top" charoff="50">RBF</th></tr></thead><tbody valign="top"><tr><td align="left" valign="top" charoff="50"><bold><italic>Computer hardware</italic></bold></td><td align="center" valign="top" charoff="50">209</td><td align="center" valign="top" charoff="50">7</td><td align="center" valign="top" charoff="50">69.62</td><td align="center" valign="top" charoff="50">63.13</td><td align="center" valign="top" charoff="50">134.91</td><td align="center" valign="top" charoff="50">159.23</td><td align="center" valign="top" charoff="50">93.63</td><td align="center" valign="top" charoff="50">91.67</td><td align="center" valign="top" charoff="50">59.66</td><td align="center" valign="top" charoff="50"><bold>58.39</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Yacht hydrodynamics</italic></bold></td><td align="center" valign="top" charoff="50">308</td><td align="center" valign="top" charoff="50">6</td><td align="center" valign="top" charoff="50">9.13</td><td align="center" valign="top" charoff="50">6.43</td><td align="center" valign="top" charoff="50">1.18</td><td align="center" valign="top" charoff="50">1.96</td><td align="center" valign="top" charoff="50">1.03</td><td align="center" valign="top" charoff="50">1.16</td><td align="center" valign="top" charoff="50"><bold>1.00</bold></td><td align="center" valign="top" charoff="50"><bold>1.00</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Housing</italic></bold></td><td align="center" valign="top" charoff="50">506</td><td align="center" valign="top" charoff="50">12</td><td align="center" valign="top" charoff="50">4.88</td><td align="center" valign="top" charoff="50">4.10</td><td align="center" valign="top" charoff="50">4.94</td><td align="center" valign="top" charoff="50">7.92</td><td align="center" valign="top" charoff="50">3.16</td><td align="center" valign="top" charoff="50">3.40</td><td align="center" valign="top" charoff="50"><bold>3.07</bold></td><td align="center" valign="top" charoff="50">3.13</td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Forest fire<xref ref-type="fn" rid="t1-fn1">*</xref></italic></bold></td><td align="center" valign="top" charoff="50">517</td><td align="center" valign="top" charoff="50">13</td><td align="center" valign="top" charoff="50">1.50</td><td align="center" valign="top" charoff="50"><bold>1.40</bold></td><td align="center" valign="top" charoff="50">2.10</td><td align="center" valign="top" charoff="50"><bold>1.40</bold></td><td align="center" valign="top" charoff="50">1.50</td><td align="center" valign="top" charoff="50"><bold>1.40</bold></td><td align="center" valign="top" charoff="50">1.41</td><td align="center" valign="top" charoff="50"><bold>1.40</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Istanbul stock exchange</italic></bold></td><td align="center" valign="top" charoff="50">536</td><td align="center" valign="top" charoff="50">8</td><td align="center" valign="top" charoff="50"><bold>0.01</bold></td><td align="center" valign="top" charoff="50">0.01</td><td align="center" valign="top" charoff="50">0.04</td><td align="center" valign="top" charoff="50">0.02</td><td align="center" valign="top" charoff="50"><bold>0.01</bold></td><td align="center" valign="top" charoff="50"><bold>0.01</bold></td><td align="center" valign="top" charoff="50"><bold>0.01</bold></td><td align="center" valign="top" charoff="50"><bold>0.01</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Concrete compressive strength</italic></bold></td><td align="center" valign="top" charoff="50">1030</td><td align="center" valign="top" charoff="50">9</td><td align="center" valign="top" charoff="50">10.53</td><td align="center" valign="top" charoff="50">8.28</td><td align="center" valign="top" charoff="50">6.36</td><td align="center" valign="top" charoff="50">13.18</td><td align="center" valign="top" charoff="50">5.25</td><td align="center" valign="top" charoff="50">4.72</td><td align="center" valign="top" charoff="50">4.53</td><td align="center" valign="top" charoff="50"><bold>4.18</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Parkinsons telemonitoring</italic></bold></td><td align="center" valign="top" charoff="50">5875</td><td align="center" valign="top" charoff="50">19</td><td align="center" valign="top" charoff="50">9.74</td><td align="center" valign="top" charoff="50">6.10</td><td align="center" valign="top" charoff="50">6.69</td><td align="center" valign="top" charoff="50">10.35</td><td align="center" valign="top" charoff="50">6.02</td><td align="center" valign="top" charoff="50">2.10</td><td align="center" valign="top" charoff="50">1.65</td><td align="center" valign="top" charoff="50"><bold>1.19</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Wine quality</italic></bold></td><td align="center" valign="top" charoff="50">6497</td><td align="center" valign="top" charoff="50">11</td><td align="center" valign="top" charoff="50">0.74</td><td align="center" valign="top" charoff="50">0.70</td><td align="center" valign="top" charoff="50">0.73</td><td align="center" valign="top" charoff="50">0.92</td><td align="center" valign="top" charoff="50">0.67</td><td align="center" valign="top" charoff="50">0.67</td><td align="center" valign="top" charoff="50">0.58</td><td align="center" valign="top" charoff="50"><bold>0.57</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Bike sharing</italic></bold></td><td align="center" valign="top" charoff="50">17389</td><td align="center" valign="top" charoff="50">16</td><td align="center" valign="top" charoff="50">141.87</td><td align="center" valign="top" charoff="50">104.58</td><td align="center" valign="top" charoff="50">65.99</td><td align="center" valign="top" charoff="50">94.56</td><td align="center" valign="top" charoff="50">102.37</td><td align="center" valign="top" charoff="50">75.47</td><td align="center" valign="top" charoff="50">39.97</td><td align="center" valign="top" charoff="50"><bold>38.26</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Buzz in social media tomhardware<xref ref-type="fn" rid="t1-fn1">*</xref></italic></bold></td><td align="center" valign="top" charoff="50">28179</td><td align="center" valign="top" charoff="50">97</td><td align="center" valign="top" charoff="50">1.45</td><td align="center" valign="top" charoff="50">0.76</td><td align="center" valign="top" charoff="50">0.37</td><td align="center" valign="top" charoff="50">1.58</td><td align="center" valign="top" charoff="50">1.49</td><td align="center" valign="top" charoff="50"><bold>0.31</bold></td><td align="center" valign="top" charoff="50"><bold>0.31</bold></td><td align="center" valign="top" charoff="50"><bold>0.31</bold></td></tr><tr><td align="left" valign="top" charoff="50"><italic><bold>Physicochemical properties</bold></italic></td><td align="center" valign="top" charoff="50">45730</td><td align="center" valign="top" charoff="50">9</td><td align="center" valign="top" charoff="50">5.19</td><td align="center" valign="top" charoff="50">3.79</td><td align="center" valign="top" charoff="50">6.12</td><td align="center" valign="top" charoff="50">6.12</td><td align="center" valign="top" charoff="50">4.16</td><td align="center" valign="top" charoff="50">5.05</td><td align="center" valign="top" charoff="50">3.45</td><td align="center" valign="top" charoff="50"><bold>3.27</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>3D Road Network</italic></bold></td><td align="center" valign="top" charoff="50">434874</td><td align="center" valign="top" charoff="50">2</td><td align="center" valign="top" charoff="50">18.37</td><td align="center" valign="top" charoff="50">6.44</td><td align="center" valign="top" charoff="50">15.55</td><td align="center" valign="top" charoff="50">16.95</td><td align="center" valign="top" charoff="50">12.53</td><td align="center" valign="top" charoff="50">14.82</td><td align="center" valign="top" charoff="50">3.86</td><td align="center" valign="top" charoff="50"><bold>1.20</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Year prediction MSD</italic></bold></td><td align="center" valign="top" charoff="50">515345</td><td align="center" valign="top" charoff="50">90</td><td align="center" valign="top" charoff="50">9.55</td><td align="center" valign="top" charoff="50">9.22</td><td align="center" valign="top" charoff="50">10.93</td><td align="center" valign="top" charoff="50">11.47</td><td align="center" valign="top" charoff="50">&#x02014;</td><td align="center" valign="top" charoff="50">9.63</td><td align="center" valign="top" charoff="50">9.24</td><td align="center" valign="top" charoff="50"><bold>8.87</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Buzz in social media twitter<xref ref-type="fn" rid="t1-fn1">*</xref></italic></bold></td><td align="center" valign="top" charoff="50">583250</td><td align="center" valign="top" charoff="50">78</td><td align="center" valign="top" charoff="50">1.33</td><td align="center" valign="top" charoff="50">0.52</td><td align="center" valign="top" charoff="50">0.51</td><td align="center" valign="top" charoff="50">1.03</td><td align="center" valign="top" charoff="50">&#x02014;</td><td align="center" valign="top" charoff="50">0.48</td><td align="center" valign="top" charoff="50"><bold>0.47</bold></td><td align="center" valign="top" charoff="50"><bold>0.47</bold></td></tr></tbody></table><table-wrap-foot><fn id="t1-fn1"><p>Bold: The bold means the first place result of all methods compared.</p></fn><fn id="t1-fn2"><p>*The * means the dependent variable of the corresponding data was transformed by log function to be more asymptotically normal.</p></fn><fn id="t1-fn3"><p>The best RBF&#x02019;s RMSE was significantly less than the second best RF using Wilcoxon Matched-Pairs Signed-Ranks Test (<italic>p-value</italic>&#x02009;=&#x02009;0.007185).</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="t2"><label>Table 2</label><caption><title>Classification error of all methods on 14 datasets.</title></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/></colgroup><thead valign="bottom"><tr><th align="left" valign="top" charoff="50">Classification error%</th><th align="center" valign="top" charoff="50">Sample</th><th align="center" valign="top" charoff="50">Feature</th><th align="center" valign="top" charoff="50">LR</th><th align="center" valign="top" charoff="50">KNN</th><th align="center" valign="top" charoff="50">NN</th><th align="center" valign="top" charoff="50">ELM</th><th align="center" valign="top" charoff="50">SVM</th><th align="center" valign="top" charoff="50">GBM</th><th align="center" valign="top" charoff="50">RF</th><th align="center" valign="top" charoff="50">RBF</th></tr></thead><tbody valign="top"><tr><td align="left" valign="top" charoff="50"><bold><italic>Fertility</italic></bold></td><td align="center" valign="top" charoff="50">100</td><td align="center" valign="top" charoff="50">9</td><td align="center" valign="top" charoff="50">15.00</td><td align="center" valign="top" charoff="50">12.00</td><td align="center" valign="top" charoff="50">15.00</td><td align="center" valign="top" charoff="50">24.00</td><td align="center" valign="top" charoff="50">12.00</td><td align="center" valign="top" charoff="50"><bold>12.00</bold></td><td align="center" valign="top" charoff="50"><bold>12.00</bold></td><td align="center" valign="top" charoff="50"><bold>12.00</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Connectionist Bench</italic></bold></td><td align="center" valign="top" charoff="50">208</td><td align="center" valign="top" charoff="50">60</td><td align="center" valign="top" charoff="50">26.00</td><td align="center" valign="top" charoff="50">13.02</td><td align="center" valign="top" charoff="50">21.67</td><td align="center" valign="top" charoff="50">14.43</td><td align="center" valign="top" charoff="50"><bold>10.14</bold></td><td align="center" valign="top" charoff="50">12.52</td><td align="center" valign="top" charoff="50">12.52</td><td align="center" valign="top" charoff="50">12.02</td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Habermans survival</italic></bold></td><td align="center" valign="top" charoff="50">306</td><td align="center" valign="top" charoff="50">3</td><td align="center" valign="top" charoff="50">25.85</td><td align="center" valign="top" charoff="50">25.16</td><td align="center" valign="top" charoff="50">30.71</td><td align="center" valign="top" charoff="50">27.40</td><td align="center" valign="top" charoff="50">26.45</td><td align="center" valign="top" charoff="50">27.12</td><td align="center" valign="top" charoff="50">27.4</td><td align="center" valign="top" charoff="50"><bold>25.12</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Ionosphere</italic></bold></td><td align="center" valign="top" charoff="50">351</td><td align="center" valign="top" charoff="50">34</td><td align="center" valign="top" charoff="50">10.26</td><td align="center" valign="top" charoff="50">10.25</td><td align="center" valign="top" charoff="50">11.98</td><td align="center" valign="top" charoff="50">10.28</td><td align="center" valign="top" charoff="50">5.13</td><td align="center" valign="top" charoff="50">6.26</td><td align="center" valign="top" charoff="50">6.55</td><td align="center" valign="top" charoff="50"><bold>4.26</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Climate Model Simulation Crashes</italic></bold></td><td align="center" valign="top" charoff="50">540</td><td align="center" valign="top" charoff="50">18</td><td align="center" valign="top" charoff="50"><bold>4.26</bold></td><td align="center" valign="top" charoff="50">7.04</td><td align="center" valign="top" charoff="50">5.56</td><td align="center" valign="top" charoff="50">5.93</td><td align="center" valign="top" charoff="50">4.44</td><td align="center" valign="top" charoff="50">5.74</td><td align="center" valign="top" charoff="50">6.48</td><td align="center" valign="top" charoff="50">4.81</td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Breast Cancer Wisconsin Diagnostic</italic></bold></td><td align="center" valign="top" charoff="50">569</td><td align="center" valign="top" charoff="50">30</td><td align="center" valign="top" charoff="50">5.09</td><td align="center" valign="top" charoff="50">2.81</td><td align="center" valign="top" charoff="50">8.45</td><td align="center" valign="top" charoff="50">8.80</td><td align="center" valign="top" charoff="50"><bold>1.93</bold></td><td align="center" valign="top" charoff="50">3.33</td><td align="center" valign="top" charoff="50">2.98</td><td align="center" valign="top" charoff="50">2.28</td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Indian Liver Patient</italic></bold></td><td align="center" valign="top" charoff="50">579</td><td align="center" valign="top" charoff="50">10</td><td align="center" valign="top" charoff="50">27.83</td><td align="center" valign="top" charoff="50">27.82</td><td align="center" valign="top" charoff="50">30.21</td><td align="center" valign="top" charoff="50">28.34</td><td align="center" valign="top" charoff="50">28.51</td><td align="center" valign="top" charoff="50">27.47</td><td align="center" valign="top" charoff="50"><bold>26.09</bold></td><td align="center" valign="top" charoff="50">26.42</td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Blood Transfusion Service Center</italic></bold></td><td align="center" valign="top" charoff="50">748</td><td align="center" valign="top" charoff="50">4</td><td align="center" valign="top" charoff="50">22.86</td><td align="center" valign="top" charoff="50"><bold>19.65</bold></td><td align="center" valign="top" charoff="50">24.46</td><td align="center" valign="top" charoff="50">23.80</td><td align="center" valign="top" charoff="50">20.19</td><td align="center" valign="top" charoff="50">21.66</td><td align="center" valign="top" charoff="50">21.79</td><td align="center" valign="top" charoff="50">19.92</td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>QSAR biodegradation</italic></bold></td><td align="center" valign="top" charoff="50">1055</td><td align="center" valign="top" charoff="50">41</td><td align="center" valign="top" charoff="50">13.37</td><td align="center" valign="top" charoff="50">13.75</td><td align="center" valign="top" charoff="50">14.98</td><td align="center" valign="top" charoff="50">22.38</td><td align="center" valign="top" charoff="50">12.14</td><td align="center" valign="top" charoff="50">12.89</td><td align="center" valign="top" charoff="50">12.42</td><td align="center" valign="top" charoff="50"><bold>11.95</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Hill valley with noise</italic></bold></td><td align="center" valign="top" charoff="50">1212</td><td align="center" valign="top" charoff="50">100</td><td align="center" valign="top" charoff="50">42.00</td><td align="center" valign="top" charoff="50">45.71</td><td align="center" valign="top" charoff="50">5.28</td><td align="center" valign="top" charoff="50">23.42</td><td align="center" valign="top" charoff="50">34.73</td><td align="center" valign="top" charoff="50">43.89</td><td align="center" valign="top" charoff="50">40.50</td><td align="center" valign="top" charoff="50"><bold>2.47</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>Banknote authentication</italic></bold></td><td align="center" valign="top" charoff="50">1372</td><td align="center" valign="top" charoff="50">4</td><td align="center" valign="top" charoff="50">1.02</td><td align="center" valign="top" charoff="50">0.15</td><td align="center" valign="top" charoff="50"><bold>0.00</bold></td><td align="center" valign="top" charoff="50"><bold>0.00</bold></td><td align="center" valign="top" charoff="50"><bold>0.00</bold></td><td align="center" valign="top" charoff="50">0.15</td><td align="center" valign="top" charoff="50">0.51</td><td align="center" valign="top" charoff="50"><bold>0.00</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>EEG Eye State</italic></bold></td><td align="center" valign="top" charoff="50">14980</td><td align="center" valign="top" charoff="50">14</td><td align="center" valign="top" charoff="50">35.75</td><td align="center" valign="top" charoff="50">15.37</td><td align="center" valign="top" charoff="50">31.57</td><td align="center" valign="top" charoff="50">42.34</td><td align="center" valign="top" charoff="50">19.52</td><td align="center" valign="top" charoff="50">8.46</td><td align="center" valign="top" charoff="50">5.96</td><td align="center" valign="top" charoff="50"><bold>3.66</bold></td></tr><tr><td align="left" valign="top" charoff="50"><bold><italic>MAGIC Gamma Telescope</italic></bold></td><td align="center" valign="top" charoff="50">19020</td><td align="center" valign="top" charoff="50">10</td><td align="center" valign="top" charoff="50">20.88</td><td align="center" valign="top" charoff="50">15.86</td><td align="center" valign="top" charoff="50">13.17</td><td align="center" valign="top" charoff="50">22.64</td><td align="center" valign="top" charoff="50">12.30</td><td align="center" valign="top" charoff="50">11.75</td><td align="center" valign="top" charoff="50">11.73</td><td align="center" valign="top" charoff="50"><bold>10.36</bold></td></tr></tbody></table><table-wrap-foot><fn id="t2-fn1"><p>Bold: The bold means the first place result of all methods compared.</p></fn><fn id="t2-fn2"><p>The best RBF&#x02019;s error% was significantly less than the second best SVM using Wilcoxon Matched-Pairs Signed-Ranks Test (<italic>p-value</italic>&#x02009;=&#x02009;0.04584).</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="t3"><label>Table 3</label><caption><title>Psoriasis prediction performance with all methods based on best number of SNP subsets.</title></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/></colgroup><thead valign="bottom"><tr><th rowspan="2" align="left" valign="top" charoff="50">&#x000a0;</th><th colspan="5" align="center" valign="top" charoff="50">Independent testing dataset (ADO dataset)<hr/></th><th colspan="5" align="center" valign="top" charoff="50">Training dataset (GRU dataset) with 10-fold cross validation<xref ref-type="fn" rid="t3-fn1">*</xref><hr/></th></tr><tr><th align="center" valign="top" charoff="50">Sensitivity</th><th align="center" valign="top" charoff="50">Specificity</th><th align="center" valign="top" charoff="50">Accuracy</th><th align="center" valign="top" charoff="50">AUC</th><th align="center" valign="top" charoff="50">95% CI of AUC</th><th align="center" valign="top" charoff="50">Sensitivity</th><th align="center" valign="top" charoff="50">Specificity</th><th align="center" valign="top" charoff="50">Accuracy</th><th align="center" valign="top" charoff="50">AUC</th><th align="center" valign="top" charoff="50">95% CI of AUC</th></tr></thead><tbody valign="top"><tr><td align="left" valign="top" charoff="50">NN</td><td align="center" valign="top" charoff="50">0.6404</td><td align="center" valign="top" charoff="50">0.5840</td><td align="center" valign="top" charoff="50">0.6055</td><td align="center" valign="top" charoff="50">0.6563</td><td align="center" valign="top" charoff="50">[0.6240,&#x000a0;0.6886]</td><td align="center" valign="top" charoff="50">0.5347</td><td align="center" valign="top" charoff="50">0.6657</td><td align="center" valign="top" charoff="50">0.5899</td><td align="center" valign="top" charoff="50">0.6192</td><td align="center" valign="top" charoff="50">[0.4388,&#x000a0;0.7893]</td></tr><tr><td align="left" valign="top" charoff="50">KNN</td><td align="center" valign="top" charoff="50">0.6241</td><td align="center" valign="top" charoff="50">0.7279</td><td align="center" valign="top" charoff="50">0.6884</td><td align="center" valign="top" charoff="50">0.7021</td><td align="center" valign="top" charoff="50">[0.6699,&#x000a0;0.7344]</td><td align="center" valign="top" charoff="50">0.6428</td><td align="center" valign="top" charoff="50">0.6553</td><td align="center" valign="top" charoff="50">0.6478</td><td align="center" valign="top" charoff="50">0.6660</td><td align="center" valign="top" charoff="50">[0.5342,&#x000a0;0.7830]</td></tr><tr><td align="left" valign="top" charoff="50">ELM</td><td align="center" valign="top" charoff="50">0.6589</td><td align="center" valign="top" charoff="50">0.6610</td><td align="center" valign="top" charoff="50">0.6602</td><td align="center" valign="top" charoff="50">0.7053</td><td align="center" valign="top" charoff="50">[0.6738,&#x000a0;0.7368]</td><td align="center" valign="top" charoff="50">0.6305</td><td align="center" valign="top" charoff="50">0.6403</td><td align="center" valign="top" charoff="50">0.6346</td><td align="center" valign="top" charoff="50">0.6618</td><td align="center" valign="top" charoff="50">[0.5210,&#x000a0;0.8094]</td></tr><tr><td align="left" valign="top" charoff="50">RF</td><td align="center" valign="top" charoff="50">0.6311</td><td align="center" valign="top" charoff="50">0.7051</td><td align="center" valign="top" charoff="50">0.6770</td><td align="center" valign="top" charoff="50">0.7134</td><td align="center" valign="top" charoff="50">[0.6820,&#x000a0;0.7448]</td><td align="center" valign="top" charoff="50">0.6036</td><td align="center" valign="top" charoff="50">0.6703</td><td align="center" valign="top" charoff="50">0.6314</td><td align="center" valign="top" charoff="50">0.6603</td><td align="center" valign="top" charoff="50">[0.5072,&#x000a0;0.7954]</td></tr><tr><td align="left" valign="top" charoff="50">SVM</td><td align="center" valign="top" charoff="50">0.6589</td><td align="center" valign="top" charoff="50">0.6952</td><td align="center" valign="top" charoff="50">0.6814</td><td align="center" valign="top" charoff="50">0.7132</td><td align="center" valign="top" charoff="50">[0.6815,&#x000a0;0.7449]</td><td align="center" valign="top" charoff="50">0.6569</td><td align="center" valign="top" charoff="50">0.6419</td><td align="center" valign="top" charoff="50">0.6503</td><td align="center" valign="top" charoff="50">0.6694</td><td align="center" valign="top" charoff="50">[0.5319,&#x000a0;0.7843]</td></tr><tr><td align="left" valign="top" charoff="50">GBM</td><td align="center" valign="top" charoff="50">0.6473</td><td align="center" valign="top" charoff="50">0.7080</td><td align="center" valign="top" charoff="50">0.6849</td><td align="center" valign="top" charoff="50">0.7187</td><td align="center" valign="top" charoff="50">[0.6873,&#x000a0;0.7500]</td><td align="center" valign="top" charoff="50">0.5890</td><td align="center" valign="top" charoff="50">0.7129</td><td align="center" valign="top" charoff="50">0.6415</td><td align="center" valign="top" charoff="50">0.6707</td><td align="center" valign="top" charoff="50">[0.5153,&#x000a0;0.7986]</td></tr><tr><td align="left" valign="top" charoff="50">RBF</td><td align="center" valign="top" charoff="50">0.6543</td><td align="center" valign="top" charoff="50">0.7151</td><td align="center" valign="top" charoff="50">0.6920</td><td align="center" valign="top" charoff="50"><bold>0.7239</bold></td><td align="center" valign="top" charoff="50">[0.6930,&#x000a0;0.7548]</td><td align="center" valign="top" charoff="50">0.6317</td><td align="center" valign="top" charoff="50">0.6490</td><td align="center" valign="top" charoff="50">0.6390</td><td align="center" valign="top" charoff="50"><bold>0.6739</bold></td><td align="center" valign="top" charoff="50">[0.5254,&#x000a0;0.8275]</td></tr></tbody></table><table-wrap-foot><fn id="t3-fn1"><p>Bold: The bold means the first place result of all methods compared. *AUC, sensitivity, specificity, and accuracy were its average value in 10-fold CV,</p></fn><fn id="t3-fn2"><p>95% CI of AUC represents the range of the 95% CI of AUC in 10-fold CV.</p></fn></table-wrap-foot></table-wrap></floats-group></article>