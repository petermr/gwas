<?xml version="1.0" encoding="UTF-8"?>
<p>We benchmarked nine methods: linear regression (Linear), logistic regression (LR), 
 <italic>k</italic>-Nearest Neighbors (kNN), neural networks (NN), support vector machines (SVM), extreme learning machines (ELM), random forests (RF), generalized boosted models (GBM), and Random Bits Forest (RBF). We used the RBF software available at 
 <ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/random-bits-forest/" xmlns:xlink="http://www.w3.org/1999/xlink">http://sourceforge.net/projects/random-bits-forest/</ext-link> and implemented the other eight methods using various R (v3.2.1) packages: stats, RWeka (v0.4-24), nnet (v7.3-8), kernlab (v0.9-19), randomForest (v4.6-10), elmNN (v1.0), and gbm (v2.1). We used ten-fold cross validation (accuracy, sensitivity, specificity and AUC) to evaluate each methodâ€™s performance. For methods sensitive to parameter selection, we manually tuned the parameters to obtain the best performance. As we chose the best handpicked parameters for each method respectively, the performance of each method based on the best parameters was comparable with each other. The results of tuning the parameters of sensitive methods on the real psoriasis genome-wide association study (GWAS) dataset were provided as 
 <xref ref-type="supplementary-material" rid="S1">Supplemental Materials 1</xref>. Benchmarking was performed on a desktop PC equipped with an AMD FX-8320 CPU and 32GB of memory. SVM, on some large-sample datasets, failed to complete benchmarking within reasonable time (1 week), so those results were left as blank.
</p>
