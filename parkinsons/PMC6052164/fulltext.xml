<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6052164</article-id><article-id pub-id-type="publisher-id">29246</article-id><article-id pub-id-type="doi">10.1038/s41598-018-29246-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Nuclear Norm Clustering: a promising alternative method for clustering tasks</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Wang</surname><given-names>Yi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6286-9340</contrib-id><name><surname>Li</surname><given-names>Yi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Qiao</surname><given-names>Chunhong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Liu</surname><given-names>Xiaoyu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Hao</surname><given-names>Meng</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Shugart</surname><given-names>Yin Yao</given-names></name><address><email>yinyao21043@gmail.com</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Xiong</surname><given-names>Momiao</given-names></name><address><email>momiao.xiong@gmail.com</email></address><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Jin</surname><given-names>Li</given-names></name><address><email>lijin@fudan.edu.cn</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0125 2443</institution-id><institution-id institution-id-type="GRID">grid.8547.e</institution-id><institution>Ministry of Education Key Laboratory of Contemporary Anthropology, </institution><institution>Department of Anthropology and Human Genetics, School of Life Sciences, Fudan University, </institution></institution-wrap>Shanghai, China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0125 2443</institution-id><institution-id institution-id-type="GRID">grid.8547.e</institution-id><institution>State Key Laboratory of Genetic Engineering, </institution><institution>Collaborative Innovation Center for Genetics and Development, School of Life Sciences, Fudan University, </institution></institution-wrap>Shanghai, China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2297 5165</institution-id><institution-id institution-id-type="GRID">grid.94365.3d</institution-id><institution>Unit on Statistical Genomics, </institution><institution>Division of Intramural Division Programs, National, Institute of Mental Health, National Institutes of Health, </institution></institution-wrap>Bethesda, MD USA </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9206 2401</institution-id><institution-id institution-id-type="GRID">grid.267308.8</institution-id><institution>Human Genetics Center, </institution><institution>School of Public Health, University of Texas Houston Health Sciences Center, </institution></institution-wrap>Houston, Texas USA </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0125 2443</institution-id><institution-id institution-id-type="GRID">grid.8547.e</institution-id><institution>Six Industrial Research Institute, </institution><institution>Fudan University, </institution></institution-wrap>Shanghai, China </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0125 2443</institution-id><institution-id institution-id-type="GRID">grid.8547.e</institution-id><institution>Human Phenome Institute, </institution><institution>Fudan University, </institution></institution-wrap>Shanghai, China </aff></contrib-group><pub-date pub-type="epub"><day>18</day><month>7</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>18</day><month>7</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>8</volume><elocation-id>10873</elocation-id><history><date date-type="received"><day>26</day><month>6</month><year>2017</year></date><date date-type="accepted"><day>2</day><month>7</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Clustering techniques are widely used in many applications. The goal of clustering is to identify patterns or groups of similar objects within a dataset of interest. However, many cluster methods are neither robust nor sensitive to noises and outliers in real data. In this paper, we present Nuclear Norm Clustering (NNC, available at https://sourceforge.net/projects/nnc/), an algorithm that can be used in various fields as a promising alternative to the k-means clustering method. The NNC algorithm requires users to provide a data matrix M and a desired number of cluster K. We employed simulated annealing techniques to choose an optimal label vector that minimizes nuclear norm of the pooled within cluster residual matrix. To evaluate the performance of the NNC algorithm, we compared the performance of both 15 public datasets and 2 genome-wide association studies (GWAS) on psoriasis, comparing our method with other classic methods. The results indicate that NNC method has a competitive performance in terms of F-score on 15 benchmarked public datasets and 2 psoriasis GWAS datasets. So NNC is a promising alternative method for clustering tasks.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100003347</institution-id><institution>Fudan University</institution></institution-wrap></funding-source><award-id>31330038</award-id><award-id>31521003</award-id><award-id>31501067</award-id><award-id>16JC1400500</award-id><award-id>2015FY111700</award-id><principal-award-recipient><name><surname>Li</surname><given-names>Yi</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Clustering is defined as grouping objects in sets. A good clustering method will generate clusters with a high intra-class similarity and a low inter-class similarity<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. There are several classic and representative clustering methods which are widely used in biological data analysis, including k-means clustering<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref></sup>, Partitioning Around Medoids (PAM)<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, hierarchical clustering (Hcluster)<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, Clustering Large Applications (CLARA)<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, Agglomerative Nesting (AGNES)<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>, Divisive Analysis Clustering (DIANA)<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, Clusterdp<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup> and DBSCAN<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>.</p><p id="Par3">K-means clustering is a popular method of vector quantization in data mining. The term &#x0201c;k-means&#x0201d; was first used by MacQueen<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> in 1967 and the standard algorithm was first proposed by Lloyd<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> in 1957. K-means clustering is typically used to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.</p><p id="Par4">The Partitioning Around Medoids (PAM) is a clustering algorithm related to the k-means clustering and the medoids shift algorithm<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Both the k-means and PAM are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means clustering, PAM chooses data points as centers and works with a generalization of the Manhattan Norm to define distance between data points. The PAM method was proposed in 1987 and is a classical partitioning technique of clustering that clusters the dataset of n objects into k clusters.</p><p id="Par5">Hierarchical clustering (Hcluster)<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two subcategories: agglomerative and divisive<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. In general, the merges and splits can be achieved in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.</p><p id="Par6">Clustering large applications (CLARA)<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> is characterized by taking a small portion of the data as a sample without considering the entire data set. It extracts multiple sample sets from the data set and uses the best cluster as output, by using PAM for each sample set. CLARA can handle a larger data set than PAM. Agglomerative nesting (AGNES)<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup> algorithm belongs to hierarchical clustering method. AGNES initially takes each object as a cluster, afterwards the clusters are merged step by step according to certain criteria, using a single-link method. The level of similarity of the two clusters is measured by the similarity of the nearest pair of data points in the two different clusters. The clustering process is repeated until all objects finally meet the number of clusters. The DIANA (Divisive analysis)<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> algorithm is a typical split clustering method. DIANA first places all objects in a cluster and then subdivides them into smaller clusters until the desired number of clusters is obtained. Density-based methods include Clusterdp<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>, DBSCAN<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, etc. Clusterdp<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup> is a recently developed method based on the idea that centroids are characterized by a higher local density than their neighbors and by a comparably high distance from objects with higher density.</p><p id="Par7">Obviously, each clustering method has its own strengths and drawbacks. Although some methods work well on one data set, it may give poor results on another data set. The K-means clustering algorithm is compromised when feature is highly correlated and is extremely sensitive to outliers, because its distance measurement can be easily influenced by extreme values, and it is also computationally difficult (NP-hard)<sup><xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR15">15</xref></sup>. The most time-consuming part of PAM is the calculation of the distances between objects. CLARA relies on the sampling approach to handle large datasets<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, therefore, the quality of CLARA&#x02019;s clustering results depends greatly on the size of the sample. AGNES algorithm does not undo what was previously carried out. No objective function is directly minimized. Sometimes it is difficult to identify the correct number of clusters by using the dendrogram. DIANA chooses the object with the maximum average dissimilarity and then moves all objects to this cluster that are more similar to the new cluster than to the remainder.</p><p id="Par8">We consider that the objective of clustering is to minimize the &#x0201c;residuals&#x0201d; within clusters. We can use norms to measure &#x0201c;residuals&#x0201d;, like L2~L0 norms<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. For example, L2 error is the square error, L1 error is the nuclear norm and L0 error is the rank of the residual matrix. Minimizing nuclear norm not only reduces the quantitative error (variance) but also reduces the qualitative errors (rank) and encourages the residuals to be embedded in low dimensional spaces. To achieve this goal, we developed the Nuclear Norm Clustering (NNC) method (available at https://sourceforge.net/projects/nnc/), a highly accurate and robust algorithm used for clustering analysis. Nuclear Norm Clustering aims to improve the accuracy of clustering. In this paper, we compared the performance of NNC with that of other seven methods, using 15 publically available datasets. We then tested the performance of NNC on two psoriasis genome-wide association study (GWAS) datasets<sup><xref ref-type="bibr" rid="CR17">17</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup>.</p></sec><sec id="Sec2"><title>Methods</title><p id="Par9">To apply our method to a specific dataset, users need to provide a data matrix M and the desired number of cluster K. The objective function to minimize is the nuclear norm of the pooled within class residual. The nuclear norm of a matrix is defined as the sum of singular values of the matrix.</p><p id="Par10">Suppose we had a candidate class label vector A, where A[i] was an integer indicating that the i<sub>th</sub> sample belong to the A[i]<sub>th</sub> cluster. We first calculated the means/center of each class. Then for each sample/row, we subtracted its corresponding class mean, forming a pooled residual matrix. Then we performed singular value decomposition (SVD)<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> to obtain nuclear norm. This procedure could be denoted as NN(A).</p><p id="Par11">We used simulated annealing<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> to choose an optimal A that minimize NN(A). First we initially random guess some A. Then we randomly change one sample&#x02019;s label obtain A&#x02032;, and test if it improves the nuclear norm. If Uniform(0, 1)&#x02009;&#x0003c;&#x02009;exp((NN-NN&#x02019;)/T) then A&#x02009;=&#x02009;A&#x02032;, where T is the annealing parameter. The algorithm is shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The pseudocode of Nuclear Norm Clustering.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td>subroutine <bold>Nuclear Norm (A, M)</bold><break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;{Parameter: assignment vector A, normalized data matrix M.<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;1: &#x000a0;&#x000a0;&#x000a0;Calculate the cluster center/mean C using A and M<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;2: &#x000a0;&#x000a0;&#x000a0;For each rows i in M, subtract the corresponding cluster mean vector C<sub>Ai</sub>.<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;3: &#x000a0;&#x000a0;&#x000a0;Perform SVD of the matrix M&#x02009;=&#x02009;USV<sup>T</sup> return the sum of singular values}</td></tr><tr><td>
<bold>Nuclear Norm Clustering of normalized data matrix M</bold>
<break/>1: &#x000a0;&#x000a0;&#x000a0;randomly assign the assignment vector A<break/>2: &#x000a0;&#x000a0;&#x000a0;NN&#x02009;=&#x02009;Nuclear Norm (A, M)<break/>3: &#x000a0;&#x000a0;&#x000a0;repeat N iterations<break/>4: &#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;{A&#x02032;&#x02009;=&#x02009;A<break/>5: &#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x02003;&#x02002;A&#x02032; [random sample]&#x02009;=&#x02009;random cluster<break/>6: &#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x02003;&#x02002;NN&#x02032;&#x02009;=&#x02009;Nuclear Norm (A&#x02032;)<break/>7: &#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x02003;&#x02002;T&#x02009;=&#x02009;N/(100*(iter&#x02009;+&#x02009;1.0))<break/>8: &#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x02003;&#x02002;if (Uniform (0, 1)&#x02009;&#x0003c;&#x02009;exp ((NN-NN&#x02032;)/T))<break/>9: &#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x02003;&#x02003;&#x02003;&#x02003;{A&#x02009;=&#x02009;A&#x02032;<break/>10: &#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x02003;&#x02003;&#x02003;&#x02003;NN&#x02009;=&#x02009;NN&#x02032;}}<break/>11: &#x000a0;&#x000a0;&#x000a0;A is the clustering result</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec3"><title>Bechmarking</title><p id="Par12">We benchmarked eight methods: k-means clustering, Partitioning Around Medoids (PAM), Hierarchical clustering (Hcluster, using Euclidean metric to calculate dissimilarities), Clustering Large Applications (CLARA), Agglomerative Nesting (AGNES), Divisive Analysis Clustering (DIANA), Clusterdp (Clusterdp was chosen as the representative of density-based methods) and Nuclear Norm Clustering (NNC). We used the NNC software available at https://sourceforge.net/projects/nnc/ and implemented the other seven methods using various R packages: factoextra<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> and densityClust<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. To evaluate the performance of benchmarked clustering methods, we used the macro-averaged F-score<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. Benchmarking was performed on a desktop PC equipped with an Intel Core i7-4790 CPU and 32 GB of memory. The parameters tested were shown in Supplemental Materials&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>, <xref rid="MOESM2" ref-type="media">2</xref> and <xref rid="MOESM4" ref-type="media">4</xref>.</p><sec id="Sec4"><title>Benchmarking Public Datasets Study</title><p id="Par13">Overall 15 public datasets were included: <italic>spambase</italic><sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, <italic>Indian liver patient</italic><sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, <italic>blood transfusion service center</italic><sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, <italic>pima Indians diabetes</italic><sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, <italic>parkinsons</italic><sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, <italic>QSAR biodegradation</italic><sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, <italic>Ionosphere</italic><sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, <italic>pathbased</italic><sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, <italic>mammographic mass</italic><sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, <italic>breast cancer wisconsin diagnostic</italic><sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, <italic>seeds</italic><sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, <italic>wine</italic><sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, <italic>jain</italic><sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, <italic>flame</italic><sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, <italic>iris</italic><sup><xref ref-type="bibr" rid="CR27">27</xref></sup>.</p></sec><sec id="Sec5"><title>Applications on GWAS Dataset Study</title><p id="Par14">We applied each of the aforementioned method to two psoriasis genome-wide association (GWAS) genetic datasets<sup><xref ref-type="bibr" rid="CR17">17</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup>. We obtained the dataset, a part of the Collaborative Association Study of Psoriasis (CASP), from the Genetic Association Information Network (GAIN) database, a partnership of the Foundation for the National Institutes of Health. The data were available at http://dbgap.ncbi.nlm.nih.gov. through dbGap accession number phs000019.v1.p1. All genotypes were filtered by checking for data quality<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. We included 1590 subjects (915 cases, 675 controls) in the general research use (GRU) group and 1133 subjects (431 cases and 702 controls) in the autoimmune disease only (ADO) group. A dermatologist diagnosed all psoriasis cases. Each participant&#x02019;s DNA was genotyped with the Perlegen 500&#x02009;K array. Both cases and controls agreed to sign the consent contract, and controls (&#x02265;18 years old) had no confounding factors relative to a known diagnosis of psoriasis.</p><p id="Par15">In our previous work<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, we found that when the number of SNPs as predictors was chosen as 50, the independent ADO (testing) dataset could reach the maximum AUC<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> (AUC&#x02009;=&#x02009;0.7063) using logistic regression prediction model. Thus we used SNP ranking methods, considering allelic association p-values (on the Psoriasis GWAS dataset of GRU group), to select top 50 associated SNPs (take 5 intervals, such as 5, 10 &#x02026;, 50, shown in Supplementary Materials&#x000a0;<xref rid="MOESM4" ref-type="media">4</xref>) and then compared the performance of different clustering methods on two Psoriasis GWAS datasets (both GRU and ADO group).</p></sec></sec><sec id="Sec6" sec-type="results"><title>Results</title><sec id="Sec7"><title>Results from public datasets</title><p id="Par16">Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> summarizes the macro-averaged F-score of all methods on 15 public datasets. NNC, together with Clusterdp and Hcluster, all performed best in 4 datasets. PAM performed optimally in 2 datasets. Following PAM, DIANA performed best only one datasets. Furthermore, we observed that the datasets in which NNC performed better were linearly separable (especially in <italic>iris, seeds</italic> and <italic>wine</italic> datasets).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Macro-averaged F-score of all methods on 15 datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Datasets</th><th>sample</th><th>feature</th><th>class</th><th>k-means</th><th>PAM</th><th>Hcluster</th><th>CLARA</th><th>AGNES</th><th>DIANA</th><th>Clusterdp</th><th>NNC</th></tr></thead><tbody><tr><td>
<italic>spambase</italic>
</td><td>4601</td><td>57</td><td>2</td><td>0.4756</td><td>0.7594</td><td>0.8257</td><td>0.3771</td><td>0.3779</td><td>0.3779</td><td>0.6088</td><td>
<bold>0.8492</bold>
</td></tr><tr><td>
<italic>Indian liver patient</italic>
</td><td>579</td><td>10</td><td>2</td><td>0.4122</td><td>0.5406</td><td>
<bold>0.6196</bold>
</td><td>0.5418</td><td>0.4163</td><td>0.4122</td><td>0.5981</td><td>0.5837</td></tr><tr><td>
<italic>blood transfusion service center</italic>
</td><td>748</td><td>4</td><td>2</td><td>0.5630</td><td>0.5710</td><td>
<bold>0.6482</bold>
</td><td>0.5849</td><td>0.4658</td><td>0.5547</td><td>0.6304</td><td>0.5554</td></tr><tr><td>
<italic>pima Indians diabetes</italic>
</td><td>768</td><td>8</td><td>2</td><td>0.5803</td><td>0.6202</td><td>0.6918</td><td>0.6169</td><td>0.4131</td><td>0.6385</td><td>0.6100</td><td>
<bold>0.7079</bold>
</td></tr><tr><td>
<italic>parkinsons</italic>
</td><td>195</td><td>22</td><td>2</td><td>0.4682</td><td>0.6748</td><td>0.7013</td><td>0.6733</td><td>0.4231</td><td>0.4073</td><td>
<bold>0.7529</bold>
</td><td>0.6376</td></tr><tr><td>
<italic>QSAR biodegradation</italic>
</td><td>1055</td><td>41</td><td>2</td><td>0.5025</td><td>0.7112</td><td>0.7119</td><td>0.6570</td><td>0.3982</td><td>0.3982</td><td>
<bold>0.7344</bold>
</td><td>0.7057</td></tr><tr><td>
<italic>Ionosphere</italic>
</td><td>351</td><td>33</td><td>2</td><td>0.7024</td><td>0.6991</td><td>
<bold>0.7076</bold>
</td><td>0.6872</td><td>0.3992</td><td>0.5004</td><td>0.6904</td><td>0.7024</td></tr><tr><td>
<italic>mammographic mass</italic>
</td><td>830</td><td>5</td><td>2</td><td>0.6774</td><td>
<bold>0.8137</bold>
</td><td>0.8067</td><td>0.8010</td><td>0.5218</td><td>0.5374</td><td>0.7976</td><td>0.7987</td></tr><tr><td>
<italic>breast cancer wisconsin diagnostic</italic>
</td><td>569</td><td>30</td><td>2</td><td>0.8268</td><td>
<bold>0.9370</bold>
</td><td>0.9181</td><td>0.9276</td><td>0.4007</td><td>0.8832</td><td>0.8552</td><td>0.9303</td></tr><tr><td>
<italic>jain</italic>
</td><td>373</td><td>2</td><td>2</td><td>0.7660</td><td>0.8369</td><td>
<bold>1.0000</bold>
</td><td>0.7974</td><td>0.9127</td><td>0.8416</td><td>0.9001</td><td>0.8636</td></tr><tr><td>
<italic>flame</italic>
</td><td>240</td><td>2</td><td>2</td><td>0.8331</td><td>0.8461</td><td>0.8962</td><td>0.8620</td><td>0.7986</td><td>0.8584</td><td>
<bold>1.0000</bold>
</td><td>0.8303</td></tr><tr><td>
<italic>pathbased</italic>
</td><td>300</td><td>2</td><td>3</td><td>0.7081</td><td>0.7270</td><td>0.7586</td><td>0.7147</td><td>0.7223</td><td>
<bold>0.7668</bold>
</td><td>0.7273</td><td>0.7270</td></tr><tr><td>
<italic>iris</italic>
</td><td>150</td><td>4</td><td>3</td><td>0.8918</td><td>0.8593</td><td>0.8841</td><td>0.8867</td><td>0.8841</td><td>0.8512</td><td>
<bold>0.8996</bold>
</td><td>0.8853</td></tr><tr><td>
<italic>seeds</italic>
</td><td>210</td><td>7</td><td>3</td><td>0.8954</td><td>0.9104</td><td>0.9290</td><td>0.9054</td><td>0.8795</td><td>0.9037</td><td>0.9286</td><td>
<bold>0.9479</bold>
</td></tr><tr><td>
<italic>wine</italic>
</td><td>178</td><td>13</td><td>3</td><td>0.7032</td><td>0.9270</td><td>0.9500</td><td>0.9425</td><td>0.5500</td><td>0.8245</td><td>0.7860</td><td>
<bold>0.9722</bold>
</td></tr></tbody></table><table-wrap-foot><p>Bold: The bold means the first place result of all methods compared.</p></table-wrap-foot></table-wrap></p><p id="Par17">And NNC performed significantly better (Wilcoxon Rank Sum test&#x02019;s p value&#x02009;&#x0003c;&#x02009;0.05, Supplemental Materials&#x000a0;<xref rid="MOESM3" ref-type="media">3</xref>) than k-means, PAM, CLARA, AGNES and DIANA in F-score on benchmarked 15 datasets. Thus NNC is a competitive method for clustering task.</p></sec><sec id="Sec8"><title>Results from psoriasis dataset study</title><p id="Par18">We benchmarked seven methods: k-means, PAM, Hcluster, CLARA, AGNES, DIANA, NNC (Clusterdp was not included was because the psoriasis data was too large, so it took too long to adjust the parameters) in the psoriasis dataset study.</p><p id="Par19">Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> presents the mean and standard deviation of each method&#x02019;s performance among 2 psoriasis GWAS datasets. The macro-averaged F-score of selected 50 top associated SNPs (take 5 intervals) were shown in Supplemental Materials&#x000a0;<xref rid="MOESM4" ref-type="media">4</xref>. In Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, we observed that NNC had the second largest mean of F-score (mean&#x02009;=&#x02009;0.5735) in psoriasis dataset of GRU group and the maximal mean of F-score (mean&#x02009;=&#x02009;0.6725) in psoriasis dataset of ADO group, and the mean differences between NNC and the next best performing method were 0.0860 and 0.0135. Additionally, in psoriasis dataset of GRU group, NNC obviously improved the F-score in the benchmarked datasets (improved clustering accuracy&#x02009;=&#x02009;18%), compared with the third best performing method. While compared to the best performing method, the clustering accuracy of NNC was reduced by 5%. In psoriasis dataset of ADO group, the clustering accuracy of NNC was improved by 2% compared to the second best performing method. And the macro-averaged F-score curves of seven methods on psoriasis dataset 1 and psoriasis dataset 2 were shown in Figs&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>, respectively. More interestingly, we found that the F-score of NNC and Hcluster in the top 50 SNPs were superior to other methods in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. In Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, the F-score of NNC was optimal. In conclusion, NCC performed well in two psoriasis datasets and appears to be superior to its competitor methods: k-means, PAM, Hcluster, CLARA, AGNES and DIANA. It is worth mentioning that NNC appeared to be more robust and less sensitive to potential outliers. Although the F-score of NNC was not the best for all datasets, it was the top performer in both the public and the psoriasis datasets.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Mean and SD of F-score on 2 psoriasis datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">methods</th><th colspan="3">Psoriasis 1</th><th colspan="3">Psoriasis 2</th></tr><tr><th>Mean 1</th><th>SD</th><th>Pvalue</th><th>Mean 2</th><th>SD</th><th>Pvalue</th></tr></thead><tbody><tr><td>k-means</td><td>0.4363</td><td>0.1155</td><td>
<bold>1.9531E-03</bold>
</td><td>0.6314</td><td>0.0316</td><td>
<bold>2.9297E-03</bold>
</td></tr><tr><td>PAM</td><td>0.4864</td><td>0.1221</td><td>
<bold>2.4414E-02</bold>
</td><td>0.6548</td><td>0.0328</td><td>6.5430E-02</td></tr><tr><td>Hcluster</td><td>
<bold>0.6006</bold>
</td><td>0.0138</td><td>9.8145E-01</td><td>0.6590</td><td>0.0214</td><td>5.3664E-02</td></tr><tr><td>CLARA</td><td>0.4875</td><td>0.1247</td><td>9.6680E-02</td><td>0.6507</td><td>0.0229</td><td>
<bold>4.8828E-03</bold>
</td></tr><tr><td>AGNES</td><td>0.3654</td><td>0.0029</td><td>
<bold>9.7656E-04</bold>
</td><td>0.5261</td><td>0.0711</td><td>
<bold>9.7656E-04</bold>
</td></tr><tr><td>DIANA</td><td>0.4340</td><td>0.1127</td><td>
<bold>9.7656E-04</bold>
</td><td>0.6119</td><td>0.0401</td><td>
<bold>4.8828E-03</bold>
</td></tr><tr><td>NNC</td><td>0.5735</td><td>0.0722</td><td>&#x02014;</td><td>
<bold>0.6725</bold>
</td><td>0.0065</td><td>&#x02014;</td></tr></tbody></table><table-wrap-foot><p>Bold: The bold means the first place result of all methods compared. SD: Standard Deviation.</p><p>The pvalue was calculated by Wilcoxon Rank Sum test (paired&#x02009;=&#x02009;TRUE, alternative&#x02009;=&#x02009;&#x0201c;greater&#x0201d;).</p></table-wrap-foot></table-wrap><fig id="Fig1"><label>Figure 1</label><caption><p>The macro-averaged F-score of selected top 50 associated SNPs on the Psoriasis GWAS dataset of GRU group.</p></caption><graphic xlink:href="41598_2018_29246_Fig1_HTML" id="d29e1733"/></fig><fig id="Fig2"><label>Figure 2</label><caption><p>The macro-averaged F-score of selected top 50 associated SNPs on the Psoriasis GWAS dataset of ADO group.</p></caption><graphic xlink:href="41598_2018_29246_Fig2_HTML" id="d29e1742"/></fig></p></sec></sec><sec id="Sec9" sec-type="discussion"><title>Discussion</title><p id="Par20">Clustering has been applied for identifying groups among the observations<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. For example, using clustering to classify patients into subgroups according to their gene expression profile in cancer research. It can be useful for identifying the molecular profile of patients with good or bad prognostic, as well as for understanding the disease.</p><p id="Par21">NNC outperforms the k-means clustering by breaking its limitations: K-means attempts to minimize the total squared error, which is sensitive to the outliers. Furthermore, k-means performed not well in datasets (like <italic>Indian liver patient</italic> and <italic>Parkinsons</italic>, Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>) with strong correlation coefficient matrixes. To overcome the limitation, we employed the nuclear norm as a measure of clustering fitness. First, nuclear norm<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> is a L1 measure of error, thus is relatively more robust than squared error. Second, in the presence of variable correlation, nuclear norm internally orthogonalizes the variables and penalizes/down-weights correlated variables.</p><p id="Par22">NNC, along with Clusterdp and Hcluster, had the best performance in more public datasets (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>). And we found that these three methods performed best on different public datasets. They could be complementary methods in different real datasets. Furthermore, we observed that the datasets in which NNC performed better were linearly separable (especially in <italic>iris, seeds</italic> and <italic>wine</italic> datasets).</p><p id="Par23">NNC has two parameters, the desired number of cluster K and the number of iterations. The greater the number of iterations, the more precise the convergence. But if the number of iterations is too large, it will affect the computing efficiency. In the psoriasis GWAS datasets, the parameters were chosen as follows: K&#x02009;=&#x02009;2, the number of iterations&#x02009;=&#x02009;200000. Generally, when the number of iterations is 20000, NNC also performs well enough (robust with the parameters). The computation complexity of NNC is O(sample number&#x02009;&#x000d7;&#x02009;feature number&#x02009;&#x000d7;&#x02009;min(sample number, feature number)&#x02009;&#x000d7;&#x02009;iterations). When there are 10k or 100k objects in the dataset, it will be rather slow. However, NNC is fast enough (Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>) to handle medium size datasets (below 10k) in practice.<table-wrap id="Tab4"><label>Table 4</label><caption><p>The detail running time comparison of all benchmarked methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Methods</th><th>Psoriasis 1 with 1590 samples<break/>(50 SNPs)</th><th>Psoriasis 2 with 1133 samples<break/>(50 SNPs)</th></tr><tr><th>Computing Time (seconds)</th><th>Computing Time (seconds)</th></tr></thead><tbody><tr><td>k-means<sup>#</sup></td><td>0.030</td><td>0.025</td></tr><tr><td>PAM<sup>#</sup></td><td>0.053</td><td>0.030</td></tr><tr><td>Hcluster<sup>#</sup></td><td>0.056</td><td>0.025</td></tr><tr><td>CLARA<sup>#</sup></td><td>
<bold>0.016</bold>
</td><td>
<bold>0.018</bold>
</td></tr><tr><td>AGNES<sup>#</sup></td><td>0.041</td><td>0.035</td></tr><tr><td>DIANA<sup>#</sup></td><td>0.084</td><td>0.036</td></tr><tr><td>NNC(iter&#x02009;=&#x02009;20, K&#x02009;=&#x02009;2)</td><td>0.016</td><td>0.012</td></tr><tr><td>NNC(iter&#x02009;=&#x02009;200, K&#x02009;=&#x02009;2)</td><td>0.053</td><td>0.024</td></tr><tr><td>NNC(iter&#x02009;=&#x02009;2000, K&#x02009;=&#x02009;2)</td><td>0.414</td><td>0.735</td></tr><tr><td>NNC(iter&#x02009;=&#x02009;20000, K&#x02009;=&#x02009;2)</td><td>4.754</td><td>7.328</td></tr><tr><td>NNC(iter&#x02009;=&#x02009;200000, K&#x02009;=&#x02009;2)</td><td>82.757</td><td>86.205</td></tr></tbody></table><table-wrap-foot><p>Bold: The bold means the first place running time of all methods compared.</p><p>Computing time: The time calculated on the processor.</p><p><sup>#</sup>Sum of 10 times computing time according to the default parameters.</p></table-wrap-foot></table-wrap></p><p id="Par24">In conclusion, we presented the Nuclear Norm Clustering (NNC) method and our work demonstrated that NNC is a rather promising alternative method for clustering in medium size datasets.</p></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec10"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2018_29246_MOESM1_ESM.pdf"><caption><p>Supplemental Materials 1</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41598_2018_29246_MOESM2_ESM.xls"><caption><p>Supplemental Materials 2</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="41598_2018_29246_MOESM3_ESM.xls"><caption><p>Supplemental Materials 3</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="41598_2018_29246_MOESM4_ESM.xls"><caption><p>Supplemental Materials 4</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p>Yi Wang, Yi Li, Chunhong Qiao, Xiaoyu Liu and Meng Hao contributed equally to this work.</p></fn><fn><p><bold>Electronic supplementary material</bold></p><p><bold>Supplementary information</bold> accompanies this paper at 10.1038/s41598-018-29246-4.</p></fn><fn><p><bold>Publisher's note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research was supported by National Science Foundation of China (31330038, 31521003 and 31501067), National Basic Research Program (2014CB541801),&#x000a0; Ministry of Science and Technology (2015FY111700),&#x000a0;Shanghai Municipal Science and Technology Major Project (2017SHZDZX01) and the 111 Project (B13016)&#x000a0;from Ministry of Education (MOE). The computations involved in this study were supported by the Fudan University High-End Computing Center. The views expressed in this presentation do not necessarily represent the views of the NIMH, NIH, HHS or the United States Government.</p></ack><notes notes-type="author-contribution"><title>Author Contributions</title><p>Y.W., Y.L. and L.J. conceived the idea, proposed the NNC method, and contributed to writing of the paper. Y.W., Y.L. and L.J. contributed the theoretical analysis. Y.W. also contributed to the development of NNC software using C++. X.Y.L., C.H.Q., M.H. and Y.L. helped maintain NNC software and used R to generate tables and figures for all simulated and real datasets. Y.L. used the R package &#x02018;ggplot2&#x02019; to plot figures. M.M.X. helped support the psoriasis GWAS dataset. X.Y.L., C.H.Q., Y.L., Y.W. and Y.Y.S. contributed to scientific discussion and manuscript writing. Y.L., Y.W., Y.Y.S. and L.J. contributed to final revision of the paper.</p></notes><notes notes-type="COI-statement"><sec id="FPar1"><title>Competing Interests</title><p id="Par25">The authors declare no competing interests.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Kassambara, A. <italic>Practical Guide to Cluster Analysis in R: Unsupervised Machine Learning</italic>. (CreateSpace Independent Publishing Platform, 2017).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">MacQueen, J. B. Some Methods for classification and Analysis of Multivariate Observations. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. University of California Press (1967)</mixed-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lloyd</surname><given-names>SP</given-names></name></person-group><article-title>Least-Squares Quantization in Pcm</article-title><source>Ieee T Inform Theory</source><year>1982</year><volume>28</volume><fpage>129</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1109/TIT.1982.1056489</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Kaufman, L. &#x00026; Rousseeuw, P. J. <italic>Finding groups in data: an introduction to cluster analysis</italic>. Vol. 344 (John Wiley &#x00026; Sons, 2009).</mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Murtagh, F. Multidimensional clustering algorithms. <italic>Compstat Lectures, Vienna: Physika Verlag, 1985</italic> (1985).</mixed-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Struyf</surname><given-names>A</given-names></name><name><surname>Hubert</surname><given-names>M</given-names></name><name><surname>Rousseeuw</surname><given-names>P</given-names></name></person-group><article-title>Clustering in an object-oriented environment</article-title><source>J Stat Softw</source><year>1997</year><volume>1</volume><fpage>1</fpage><lpage>30</lpage></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Struyf</surname><given-names>A</given-names></name><name><surname>Hubert</surname><given-names>M</given-names></name><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name></person-group><article-title>Integrating robust clustering techniques in S-PLUS</article-title><source>Computational Statistics &#x00026; Data Analysis</source><year>1997</year><volume>26</volume><fpage>17</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1016/S0167-9473(97)00020-0</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname><given-names>A</given-names></name><name><surname>Laio</surname><given-names>A</given-names></name></person-group><article-title>Machine learning</article-title><source>Clustering by fast search and find of density peaks. Science</source><year>2014</year><volume>344</volume><fpage>1492</fpage><lpage>1496</lpage><?supplied-pmid 24970081?><pub-id pub-id-type="pmid">24970081</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiwie</surname><given-names>C</given-names></name><name><surname>Baumbach</surname><given-names>J</given-names></name><name><surname>Rottger</surname><given-names>R</given-names></name></person-group><article-title>Comparing the performance of biomedical clustering methods</article-title><source>Nat Methods</source><year>2015</year><volume>12</volume><fpage>1033</fpage><lpage>1038</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3583</pub-id><?supplied-pmid 26389570?><pub-id pub-id-type="pmid">26389570</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Martin, E., Hans-Peter, K., J&#x000f6;rg, S. &#x00026; Xiaowei, X. A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96). AAAI Press: Simoudis, Evangelos. Han, Jiawei. Fayyad, Usama M (1996)</mixed-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garey</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>D</given-names></name><name><surname>Witsenhausen</surname><given-names>H</given-names></name></person-group><article-title>The complexity of the generalized Lloyd-max problem (corresp.)</article-title><source>Ieee T Inform Theory</source><year>1982</year><volume>28</volume><fpage>255</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1109/TIT.1982.1056488</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinberg</surname><given-names>J</given-names></name><name><surname>Papadimitriou</surname><given-names>C</given-names></name><name><surname>Raghavan</surname><given-names>P</given-names></name></person-group><article-title>A microeconomic view of data mining</article-title><source>Data Min Knowl Disc</source><year>1998</year><volume>2</volume><fpage>311</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1023/A:1009726428407</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aloise</surname><given-names>D</given-names></name><name><surname>Deshpande</surname><given-names>A</given-names></name><name><surname>Hansen</surname><given-names>P</given-names></name><name><surname>Popat</surname><given-names>P</given-names></name></person-group><article-title>NP-hardness of Euclidean sum-of-squares clustering</article-title><source>Machine Learning</source><year>2009</year><volume>75</volume><fpage>245</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1007/s10994-009-5103-0</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahajan</surname><given-names>M</given-names></name><name><surname>Nimbhorkar</surname><given-names>P</given-names></name><name><surname>Varadarajan</surname><given-names>K</given-names></name></person-group><article-title>The planar k-means problem is NP-hard</article-title><source>Theor Comput Sci</source><year>2012</year><volume>442</volume><fpage>13</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.tcs.2010.05.034</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dasgupta</surname><given-names>S</given-names></name><name><surname>Freund</surname><given-names>Y</given-names></name></person-group><article-title>Random Projection Trees for Vector Quantization</article-title><source>Ieee T Inform Theory</source><year>2009</year><volume>55</volume><fpage>3229</fpage><lpage>3242</lpage><pub-id pub-id-type="doi">10.1109/TIT.2009.2021326</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Rolewicz, S. <italic>Functional analysis and control theory: Linear systems</italic>. Vol. 29 (Springer Science &#x00026; Business Media, 2013).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>S</given-names></name><name><surname>Fang</surname><given-names>X</given-names></name><name><surname>Xiong</surname><given-names>M</given-names></name></person-group><article-title>Psoriasis prediction from genome-wide SNP profiles</article-title><source>BMC Dermatol</source><year>2011</year><volume>11</volume><fpage>1</fpage><pub-id pub-id-type="doi">10.1186/1471-5945-11-1</pub-id><?supplied-pmid 21214922?><pub-id pub-id-type="pmid">21214922</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Random Bits Forest: a Strong Classifier/Regressor forBig Data</article-title><source>Scientific reports</source><year>2016</year><volume>6</volume><fpage>30086</fpage><pub-id pub-id-type="doi">10.1038/srep30086</pub-id><?supplied-pmid 27444562?><pub-id pub-id-type="pmid">27444562</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>RP</given-names></name><etal/></person-group><article-title>Sequence and haplotype analysis supports HLA-C as the psoriasis susceptibility 1 gene</article-title><source>American Journal of Human Genetics</source><year>2006</year><volume>78</volume><fpage>827</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1086/503821</pub-id><?supplied-pmid 16642438?><pub-id pub-id-type="pmid">16642438</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Xiong</surname><given-names>M</given-names></name><name><surname>Shugart</surname><given-names>YY</given-names></name><name><surname>Jin</surname><given-names>L</given-names></name></person-group><article-title>Random bits regression: a strong general predictor for big data</article-title><source>Big Data Analytics</source><year>2016</year><volume>1</volume><fpage>12</fpage><pub-id pub-id-type="doi">10.1186/s41044-016-0010-4</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alter</surname><given-names>O</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name></person-group><article-title>Singular value decomposition for genome-wide expression data processing and modeling</article-title><source>Proc Natl Acad Sci USA</source><year>2000</year><volume>97</volume><fpage>10101</fpage><lpage>10106</lpage><pub-id pub-id-type="doi">10.1073/pnas.97.18.10101</pub-id><?supplied-pmid 10963673?><pub-id pub-id-type="pmid">10963673</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkpatrick</surname><given-names>S</given-names></name><name><surname>Gelatt</surname><given-names>CD</given-names><suffix>Jr</suffix></name><name><surname>Vecchi</surname><given-names>MP</given-names></name></person-group><article-title>Optimization by simulated annealing</article-title><source>Science</source><year>1983</year><volume>220</volume><fpage>671</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1126/science.220.4598.671</pub-id><?supplied-pmid 17813860?><pub-id pub-id-type="pmid">17813860</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Kassambara, A. &#x00026; Mundt, F. Factoextra: extract and visualize the results of multivariate data analyses. <italic>R package version</italic><bold>1</bold> (2016).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Pedersen, T. &#x00026; Hughes, S. Densityclust: Clustering by Fast Search and Find of Density Peaks. <italic>R package version</italic><bold>0.2</bold> (2016).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Van Rijsbergen, C. Information retrieval. dept. of computer science, university of glasgow. <italic>URL: citeseer. ist. psu. edu/vanrijsbergen79information. html</italic><bold>14</bold> (1979).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Powers, D. M. Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation (2011).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Blake, C. L., &#x00026; Merz, C. J. UCI Repository of machine learning databases. Irvine, CA: University of California. Department of Information and Computer Science, <bold>55</bold> (1998).</mixed-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramana</surname><given-names>BV</given-names></name><name><surname>Babu</surname><given-names>MP</given-names></name><name><surname>Venkateswarlu</surname><given-names>N</given-names></name></person-group><article-title>A critical comparative study of liver patients from USA and INDIA: an exploratory analysis</article-title><source>International Journal of Computer Science Issues</source><year>2012</year><volume>9</volume><fpage>506</fpage><lpage>516</lpage></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>I-C</given-names></name><name><surname>Yang</surname><given-names>K-J</given-names></name><name><surname>Ting</surname><given-names>T-M</given-names></name></person-group><article-title>Knowledge discovery on RFM model using Bernoulli sequence</article-title><source>Expert Systems with Applications</source><year>2009</year><volume>36</volume><fpage>5866</fpage><lpage>5871</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2008.07.018</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sigillito</surname><given-names>VG</given-names></name><name><surname>Wing</surname><given-names>SP</given-names></name><name><surname>Hutton</surname><given-names>LV</given-names></name><name><surname>Baker</surname><given-names>KB</given-names></name></person-group><article-title>Classification of radar returns from the ionosphere using neural networks</article-title><source>Johns Hopkins APL Technical Digest</source><year>1989</year><volume>10</volume><fpage>262</fpage><lpage>266</lpage></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Little</surname><given-names>MA</given-names></name><name><surname>McSharry</surname><given-names>PE</given-names></name><name><surname>Roberts</surname><given-names>SJ</given-names></name><name><surname>Costello</surname><given-names>DA</given-names></name><name><surname>Moroz</surname><given-names>IM</given-names></name></person-group><article-title>Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection</article-title><source>Biomed Eng Online</source><year>2007</year><volume>6</volume><fpage>23</fpage><pub-id pub-id-type="doi">10.1186/1475-925X-6-23</pub-id><?supplied-pmid 17594480?><pub-id pub-id-type="pmid">17594480</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansouri</surname><given-names>K</given-names></name><name><surname>Ringsted</surname><given-names>T</given-names></name><name><surname>Ballabio</surname><given-names>D</given-names></name><name><surname>Todeschini</surname><given-names>R</given-names></name><name><surname>Consonni</surname><given-names>V</given-names></name></person-group><article-title>Quantitative structure-activity relationship models for ready biodegradability of chemicals</article-title><source>Journal of chemical information and modeling</source><year>2013</year><volume>53</volume><fpage>867</fpage><lpage>878</lpage><pub-id pub-id-type="doi">10.1021/ci4000213</pub-id><?supplied-pmid 23469921?><pub-id pub-id-type="pmid">23469921</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>H</given-names></name><name><surname>Yeung</surname><given-names>D-Y</given-names></name></person-group><article-title>Robust path-based spectral clustering</article-title><source>Pattern Recognition</source><year>2008</year><volume>41</volume><fpage>191</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2007.04.010</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elter</surname><given-names>M</given-names></name><name><surname>Schulz&#x02010;Wendtland</surname><given-names>R</given-names></name><name><surname>Wittenberg</surname><given-names>T</given-names></name></person-group><article-title>The prediction of breast cancer biopsy outcomes using two CAD approaches that both emphasize an intelligible decision process</article-title><source>Medical physics</source><year>2007</year><volume>34</volume><fpage>4164</fpage><lpage>4172</lpage><pub-id pub-id-type="doi">10.1118/1.2786864</pub-id><?supplied-pmid 18072480?><pub-id pub-id-type="pmid">18072480</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolberg</surname><given-names>WH</given-names></name><name><surname>Street</surname><given-names>WN</given-names></name><name><surname>Mangasarian</surname><given-names>OL</given-names></name></person-group><article-title>Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates</article-title><source>Cancer Lett</source><year>1994</year><volume>77</volume><fpage>163</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/0304-3835(94)90099-X</pub-id><?supplied-pmid 8168063?><pub-id pub-id-type="pmid">8168063</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Charytanowicz, M. <italic>et al</italic>. In <italic>Information technologies in biomedicine</italic> 15&#x02013;24 (Springer, 2010).</mixed-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>AK</given-names></name><name><surname>Law</surname><given-names>MH</given-names></name></person-group><article-title>Data clustering: A user&#x02019;s dilemma</article-title><source>PReMI</source><year>2005</year><volume>3776</volume><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Medico</surname><given-names>E</given-names></name></person-group><article-title>FLAME, a novel fuzzy clustering method for the analysis of DNA microarray data</article-title><source>BMC Bioinformatics</source><year>2007</year><volume>8</volume><fpage>3</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-8-3</pub-id><?supplied-pmid 17204155?><pub-id pub-id-type="pmid">17204155</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeLong</surname><given-names>ER</given-names></name><name><surname>DeLong</surname><given-names>DM</given-names></name><name><surname>Clarke-Pearson</surname><given-names>DL</given-names></name></person-group><article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title><source>Biometrics</source><year>1988</year><volume>44</volume><fpage>837</fpage><lpage>845</lpage><pub-id pub-id-type="doi">10.2307/2531595</pub-id><?supplied-pmid 3203132?><pub-id pub-id-type="pmid">3203132</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recht</surname><given-names>B</given-names></name><name><surname>Fazel</surname><given-names>M</given-names></name><name><surname>Parrilo</surname><given-names>PA</given-names></name></person-group><article-title>Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization</article-title><source>SIAM review</source><year>2010</year><volume>52</volume><fpage>471</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1137/070697835</pub-id></element-citation></ref></ref-list></back></article>